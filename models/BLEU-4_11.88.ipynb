{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1055e163",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/skripsi_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully logged into Hugging Face Hub.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from transformers import AutoProcessor, CLIPVisionModel\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from huggingface_hub import login\n",
    "import urllib.request\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "import math\n",
    "\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "# The Colab-specific import is no longer needed\n",
    "# from google.colab import userdata \n",
    "\n",
    "# -----------------\n",
    "# Get the token from the local environment variable\n",
    "# Set the token directly as a string (temporary and less secure)\n",
    "HUGGINGFACE_TOKEN = \"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\"\n",
    "# # Then run:\n",
    "# login(HUGGINGFACE_TOKEN)\n",
    "\n",
    "# Check if the token was found and log in\n",
    "if HUGGINGFACE_TOKEN:\n",
    "    login(HUGGINGFACE_TOKEN)\n",
    "    print(\"Successfully logged into Hugging Face Hub.\")\n",
    "else:\n",
    "    print(\"HUGGINGFACE_TOKEN environment variable not found. Please set it in your terminal.\")\n",
    "\n",
    "# The token itself should NOT be hardcoded in the script:\n",
    "# hf_cJN...jTti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "762f2ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA tersedia. GPU yang terdeteksi: NVIDIA GeForce RTX 4070 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA tersedia. GPU yang terdeteksi: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"CUDA tidak tersedia. Menggunakan CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae82d725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Membaca caption dari: /home/ubuntu/skripsi/indonesia/dataset-all-new.csv\n",
      "\n",
      "--- Info CSV ---\n",
      "Lima baris pertama data:\n",
      "  Image_name  caption_number  \\\n",
      "0   F001.jpg               0   \n",
      "1   F001.jpg               1   \n",
      "2   F001.jpg               2   \n",
      "3   F001.jpg               3   \n",
      "4   F001.jpg               4   \n",
      "\n",
      "                                             caption  \n",
      "0  Sepeda motor terparkir rapi di area parkir, se...  \n",
      "1  Sepeda motor terparkir dengan rapi di area par...  \n",
      "2  lahan parkir di suatu gedung yang diisi bebera...  \n",
      "3  Area parkir sepeda motor dengan rantai pembata...  \n",
      "4  Banyak motor terparkir di sebelah kiri dan beb...  \n",
      "\n",
      "Kolom yang ditemukan: ['Image_name', 'caption_number', 'caption']\n",
      "\n",
      "✅ Total jumlah caption (baris): 16452\n",
      "✅ Total gambar unik di CSV: 3001\n",
      "\n",
      "Menghitung file gambar di: /home/ubuntu/skripsi/indonesia/original\n",
      "✅ Total file gambar ditemukan di disk: 3001\n",
      "\n",
      "--- Validasi Data ---\n",
      "✅ Semua gambar di CSV memiliki file di disk.\n",
      "✅ Semua file gambar di disk memiliki data di CSV.\n",
      "\n",
      "--- Preprocessing Count Selesai ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# --- 1. Tentukan Path Anda ---\n",
    "IMAGES_FILE_PATH = \"/home/ubuntu/skripsi/indonesia/original\"\n",
    "CAPTIONS_FILE_PATH = \"/home/ubuntu/skripsi/indonesia/dataset-all-new.csv\"\n",
    "\n",
    "# --- 2. Baca File CSV Captions ---\n",
    "print(f\"Membaca caption dari: {CAPTIONS_FILE_PATH}\")\n",
    "try:\n",
    "    # --- PERBAIKAN: Menambahkan sep='|' ---\n",
    "    df = pd.read_csv(CAPTIONS_FILE_PATH, sep='|')\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: File CSV tidak ditemukan di {CAPTIONS_FILE_PATH}\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"ERROR saat membaca CSV: {e}\")\n",
    "    exit()\n",
    "\n",
    "print(\"\\n--- Info CSV ---\")\n",
    "print(f\"Lima baris pertama data:\\n{df.head()}\")\n",
    "print(f\"\\nKolom yang ditemukan: {df.columns.tolist()}\")\n",
    "\n",
    "# --- PERBAIKAN: Menyesuaikan nama kolom sesuai data Anda ---\n",
    "IMAGE_COLUMN_NAME = 'Image_name'\n",
    "CAPTION_COLUMN_NAME = 'caption' \n",
    "\n",
    "if IMAGE_COLUMN_NAME not in df.columns or CAPTION_COLUMN_NAME not in df.columns:\n",
    "    print(f\"ERROR: Pastikan kolom '{IMAGE_COLUMN_NAME}' dan '{CAPTION_COLUMN_NAME}' ada di CSV Anda.\")\n",
    "    exit()\n",
    "\n",
    "# --- 3. Hitung Jumlah Caption ---\n",
    "total_captions = len(df)\n",
    "unique_images_in_csv = df[IMAGE_COLUMN_NAME].nunique()\n",
    "image_names_in_csv = set(df[IMAGE_COLUMN_NAME])\n",
    "\n",
    "print(f\"\\n✅ Total jumlah caption (baris): {total_captions}\")\n",
    "print(f\"✅ Total gambar unik di CSV: {unique_images_in_csv}\")\n",
    "\n",
    "# --- 4. Hitung File Gambar di Disk ---\n",
    "print(f\"\\nMenghitung file gambar di: {IMAGES_FILE_PATH}\")\n",
    "\n",
    "# Cari semua file gambar dengan ekstensi umum\n",
    "image_extensions = ['*.jpg', '*.jpeg', '*.png']\n",
    "image_files_on_disk = []\n",
    "for ext in image_extensions:\n",
    "    image_files_on_disk.extend(glob.glob(os.path.join(IMAGES_FILE_PATH, ext)))\n",
    "\n",
    "total_image_files = len(image_files_on_disk)\n",
    "# Ambil hanya nama filenya (mis: 'gambar1.jpg')\n",
    "image_filenames_on_disk = set(os.path.basename(f) for f in image_files_on_disk)\n",
    "\n",
    "print(f\"✅ Total file gambar ditemukan di disk: {total_image_files}\")\n",
    "\n",
    "# --- 5. Validasi Data (Langkah Preprocessing Kunci) ---\n",
    "print(\"\\n--- Validasi Data ---\")\n",
    "\n",
    "# Validasi 1: Gambar di CSV tapi tidak ada filenya\n",
    "missing_image_files = image_names_in_csv - image_filenames_on_disk\n",
    "if len(missing_image_files) > 0:\n",
    "    print(f\"⚠️ Ditemukan {len(missing_image_files)} gambar di CSV yang tidak ada filenya di disk.\")\n",
    "    # Tampilkan 5 contoh pertama\n",
    "    print(f\"   Contoh: {list(missing_image_files)[:5]}\")\n",
    "else:\n",
    "    print(\"✅ Semua gambar di CSV memiliki file di disk.\")\n",
    "\n",
    "# Validasi 2: File gambar ada tapi tidak ada captionnya\n",
    "unused_image_files = image_filenames_on_disk - image_names_in_csv\n",
    "if len(unused_image_files) > 0:\n",
    "    print(f\"⚠️ Ditemukan {len(unused_image_files)} file gambar di disk yang tidak ada di CSV.\")\n",
    "    # Tampilkan 5 contoh pertama\n",
    "    print(f\"   Contoh: {list(unused_image_files)[:5]}\")\n",
    "else:\n",
    "    print(\"✅ Semua file gambar di disk memiliki data di CSV.\")\n",
    "\n",
    "print(\"\\n--- Preprocessing Count Selesai ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11916ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /home/ubuntu/anaconda3/envs/skripsi_env/lib/python3.11/site-packages (2.3.3)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.7.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /home/ubuntu/anaconda3/envs/skripsi_env/lib/python3.11/site-packages (from pandas) (2.3.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ubuntu/anaconda3/envs/skripsi_env/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ubuntu/anaconda3/envs/skripsi_env/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ubuntu/anaconda3/envs/skripsi_env/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Collecting scipy>=1.8.0 (from scikit-learn)\n",
      "  Downloading scipy-1.16.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (62 kB)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/ubuntu/anaconda3/envs/skripsi_env/lib/python3.11/site-packages (from scikit-learn) (1.5.2)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: six>=1.5 in /home/ubuntu/anaconda3/envs/skripsi_env/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading scikit_learn-1.7.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m  \u001b[33m0:00:07\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.16.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.9/35.9 MB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m  \u001b[33m0:00:34\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, scikit-learn\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [scikit-learn][0m [scikit-learn]\n",
      "\u001b[1A\u001b[2KSuccessfully installed scikit-learn-1.7.2 scipy-1.16.3 threadpoolctl-3.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edfa7b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Membaca data dari: /home/ubuntu/skripsi/indonesia/dataset-all-new.csv\n",
      "Total gambar unik ditemukan: 3001\n",
      "\n",
      "--- Hasil Pembagian Gambar (80/10/10) ---\n",
      "Total Train images: 2400\n",
      "Total Val images:   300\n",
      "Total Test images:  301\n",
      "Total:              3001\n",
      "\n",
      "Memfilter DataFrame caption...\n",
      "\n",
      "--- Jumlah Caption per Set ---\n",
      "Caption Train: 13154\n",
      "Caption Val:   1639\n",
      "Caption Test:  1659\n",
      "Total Caption: 16452\n",
      "\n",
      "Berhasil disimpan:\n",
      "Data Train: /home/ubuntu/skripsi/indonesia/train_captions.csv\n",
      "Data Val:   /home/ubuntu/skripsi/indonesia/val_captions.csv\n",
      "Data Test:  /home/ubuntu/skripsi/indonesia/test_captions.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "# --- 1. Konfigurasi ---\n",
    "BASE_DIR = \"/home/ubuntu/skripsi/indonesia\"\n",
    "CAPTIONS_FILE_PATH = os.path.join(BASE_DIR, \"dataset-all-new.csv\")\n",
    "RANDOM_SEED = 42 # Untuk memastikan hasil split bisa diulang\n",
    "\n",
    "print(f\"Membaca data dari: {CAPTIONS_FILE_PATH}\")\n",
    "df = pd.read_csv(CAPTIONS_FILE_PATH, sep='|')\n",
    "\n",
    "# --- 2. Dapatkan Daftar Gambar Unik ---\n",
    "unique_images = df['Image_name'].unique()\n",
    "print(f\"Total gambar unik ditemukan: {len(unique_images)}\")\n",
    "\n",
    "# --- 3. Split Pertama: 80% Train, 20% Sisa (untuk Val + Test) ---\n",
    "# test_size=0.2 berarti 20% untuk sisa\n",
    "train_images, temp_images = train_test_split(\n",
    "    unique_images,\n",
    "    test_size=0.2,\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "# --- 4. Split Kedua: Bagi 20% Sisa menjadi 10% Val dan 10% Test ---\n",
    "# (0.5 dari 0.2 adalah 0.1 atau 10% dari total)\n",
    "val_images, test_images = train_test_split(\n",
    "    temp_images,\n",
    "    test_size=0.5, \n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "# --- 5. Cetak Hasil Pembagian Gambar ---\n",
    "print(\"\\n--- Hasil Pembagian Gambar (80/10/10) ---\")\n",
    "print(f\"Total Train images: {len(train_images)}\")\n",
    "print(f\"Total Val images:   {len(val_images)}\")\n",
    "print(f\"Total Test images:  {len(test_images)}\")\n",
    "print(f\"Total:              {len(train_images) + len(val_images) + len(test_images)}\")\n",
    "\n",
    "# --- 6. Buat DataFrame Baru Berdasarkan Split ---\n",
    "print(\"\\nMemfilter DataFrame caption...\")\n",
    "# Menggunakan set() mempercepat pencarian\n",
    "train_img_set = set(train_images)\n",
    "val_img_set = set(val_images)\n",
    "test_img_set = set(test_images)\n",
    "\n",
    "# Filter dataframe utama\n",
    "train_df = df[df['Image_name'].isin(train_img_set)].reset_index(drop=True)\n",
    "val_df = df[df['Image_name'].isin(val_img_set)].reset_index(drop=True)\n",
    "test_df = df[df['Image_name'].isin(test_img_set)].reset_index(drop=True)\n",
    "\n",
    "print(\"\\n--- Jumlah Caption per Set ---\")\n",
    "print(f\"Caption Train: {len(train_df)}\")\n",
    "print(f\"Caption Val:   {len(val_df)}\")\n",
    "print(f\"Caption Test:  {len(test_df)}\")\n",
    "print(f\"Total Caption: {len(train_df) + len(val_df) + len(test_df)}\")\n",
    "\n",
    "# --- 7. Simpan ke File CSV Baru ---\n",
    "train_csv_path = os.path.join(BASE_DIR, \"train_captions.csv\")\n",
    "val_csv_path = os.path.join(BASE_DIR, \"val_captions.csv\")\n",
    "test_csv_path = os.path.join(BASE_DIR, \"test_captions.csv\")\n",
    "\n",
    "train_df.to_csv(train_csv_path, index=False, sep='|')\n",
    "val_df.to_csv(val_csv_path, index=False, sep='|')\n",
    "test_df.to_csv(test_csv_path, index=False, sep='|')\n",
    "\n",
    "print(f\"\\nBerhasil disimpan:\")\n",
    "print(f\"Data Train: {train_csv_path}\")\n",
    "print(f\"Data Val:   {val_csv_path}\")\n",
    "print(f\"Data Test:  {test_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d5228e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /home/ubuntu/anaconda3/envs/skripsi_env/lib/python3.11/site-packages (4.67.1)\n",
      "Requirement already satisfied: accelerate in /home/ubuntu/anaconda3/envs/skripsi_env/lib/python3.11/site-packages (1.10.1)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /home/ubuntu/anaconda3/envs/skripsi_env/lib/python3.11/site-packages (from accelerate) (2.3.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ubuntu/anaconda3/envs/skripsi_env/lib/python3.11/site-packages (from accelerate) (25.0)\n",
      "Requirement already satisfied: psutil in /home/ubuntu/anaconda3/envs/skripsi_env/lib/python3.11/site-packages (from accelerate) (7.1.0)\n",
      "Requirement already satisfied: pyyaml in /home/ubuntu/anaconda3/envs/skripsi_env/lib/python3.11/site-packages (from accelerate) (6.0.3)\n",
      "Requirement already satisfied: torch>=2.0.0 in /home/ubuntu/anaconda3/envs/skripsi_env/lib/python3.11/site-packages (from accelerate) (2.9.0)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in /home/ubuntu/anaconda3/envs/skripsi_env/lib/python3.11/site-packages (from accelerate) (0.35.3)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/ubuntu/anaconda3/envs/skripsi_env/lib/python3.11/site-packages (from accelerate) (0.6.2)\n",
      "Requirement already satisfied: filelock in /home/ubuntu/anaconda3/envs/skripsi_env/lib/python3.11/site-packages (from huggingface_hub>=0.21.0->accelerate) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ubuntu/anaconda3/envs/skripsi_env/lib/python3.11/site-packages (from huggingface_hub>=0.21.0->accelerate) (2025.10.0)\n",
      "Requirement already satisfied: requests in /home/ubuntu/anaconda3/envs/skripsi_env/lib/python3.11/site-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ubuntu/anaconda3/envs/skripsi_env/lib/python3.11/site-packages (from huggingface_hub>=0.21.0->accelerate) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/ubuntu/anaconda3/envs/skripsi_env/lib/python3.11/site-packages (from huggingface_hub>=0.21.0->accelerate) (1.1.10)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/ubuntu/anaconda3/envs/skripsi_env/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /home/ubuntu/anaconda3/envs/skripsi_env/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (3.5)\n",
      "Requirement already satisfied: jinja2 in /home/ubuntu/anaconda3/envs/skripsi_env/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/ubuntu/anaconda3/envs/skripsi_env/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/ubuntu/anaconda3/envs/skripsi_env/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/ubuntu/anaconda3/envs/skripsi_env/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/ubuntu/anaconda3/envs/skripsi_env/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/ubuntu/anaconda3/envs/skripsi_env/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/ubuntu/anaconda3/envs/skripsi_env/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/ubuntu/anaconda3/envs/skripsi_env/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/ubuntu/anaconda3/envs/skripsi_env/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/ubuntu/anaconda3/envs/skripsi_env/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/ubuntu/anaconda3/envs/skripsi_env/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /home/ubuntu/anaconda3/envs/skripsi_env/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /home/ubuntu/anaconda3/envs/skripsi_env/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/ubuntu/anaconda3/envs/skripsi_env/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/ubuntu/anaconda3/envs/skripsi_env/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/ubuntu/anaconda3/envs/skripsi_env/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.0 in /home/ubuntu/anaconda3/envs/skripsi_env/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ubuntu/anaconda3/envs/skripsi_env/lib/python3.11/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ubuntu/anaconda3/envs/skripsi_env/lib/python3.11/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ubuntu/anaconda3/envs/skripsi_env/lib/python3.11/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ubuntu/anaconda3/envs/skripsi_env/lib/python3.11/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ubuntu/anaconda3/envs/skripsi_env/lib/python3.11/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/anaconda3/envs/skripsi_env/lib/python3.11/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2025.10.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tqdm accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f67450e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/skripsi_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Menggunakan device: cuda\n",
      "Mendefinisikan arsitektur model...\n",
      "Mendefinisikan Dataset Loader...\n",
      "Inisialisasi model, data loader, dan optimizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.57s/it]\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Train: 13154 caption (loader: 1645 batch)\n",
      "Data Val:   1639 caption (loader: 205 batch)\n",
      "Mendeteksi checkpoint di /home/ubuntu/skripsi/indonesia/gemma_adapter_mlp_checkpoint.pt. Memuat...\n",
      "Checkpoint dimuat. Melanjutkan dari Epoch 8, Step 12000\n",
      "--- Memulai Training ---\n",
      "\n",
      "--- Epoch 8/10 ---\n",
      "Melanjutkan epoch 8 dari step 485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8:  60%|█████▉    | 984/1645 [1:10:41<2:01:42, 11.05s/it, train_loss=1.5391]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Menyimpan checkpoint di Step 12500...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8:  60%|█████▉    | 985/1645 [1:10:41<1:57:22, 10.67s/it, train_loss=1.5391]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint disimpan ke /home/ubuntu/skripsi/indonesia/gemma_adapter_mlp_checkpoint.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8:  90%|█████████ | 1484/1645 [2:40:10<20:12,  7.53s/it, train_loss=1.5000]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Menyimpan checkpoint di Step 13000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8:  90%|█████████ | 1485/1645 [2:40:10<20:28,  7.68s/it, train_loss=1.5000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint disimpan ke /home/ubuntu/skripsi/indonesia/gemma_adapter_mlp_checkpoint.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8: 100%|██████████| 1645/1645 [3:00:52<00:00,  6.60s/it, train_loss=0.7734]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Rata-rata Train Loss: 0.9577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validasi Epoch 8: 100%|██████████| 205/205 [05:54<00:00,  1.73s/it, val_loss=2.5938]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Rata-rata Validation Loss: 2.4369\n",
      "Validation Loss tidak membaik.\n",
      "\n",
      "--- Epoch 9/10 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9:  21%|██        | 339/1645 [44:10<3:24:23,  9.39s/it, train_loss=1.6484]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Menyimpan checkpoint di Step 13500...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9:  21%|██        | 340/1645 [44:10<3:13:03,  8.88s/it, train_loss=1.6484]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint disimpan ke /home/ubuntu/skripsi/indonesia/gemma_adapter_mlp_checkpoint.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9:  51%|█████     | 840/1645 [1:48:24<1:45:59,  7.90s/it, train_loss=1.3281]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Menyimpan checkpoint di Step 14000...\n",
      "Checkpoint disimpan ke /home/ubuntu/skripsi/indonesia/gemma_adapter_mlp_checkpoint.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9:  81%|████████▏ | 1340/1645 [2:53:01<39:52,  7.84s/it, train_loss=1.2109]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Menyimpan checkpoint di Step 14500...\n",
      "Checkpoint disimpan ke /home/ubuntu/skripsi/indonesia/gemma_adapter_mlp_checkpoint.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9: 100%|██████████| 1645/1645 [3:32:08<00:00,  7.74s/it, train_loss=1.2188]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Rata-rata Train Loss: 1.3615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validasi Epoch 9: 100%|██████████| 205/205 [05:44<00:00,  1.68s/it, val_loss=2.5469]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Rata-rata Validation Loss: 2.3822\n",
      "Validation Loss tidak membaik.\n",
      "\n",
      "--- Epoch 10/10 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10:  12%|█▏        | 195/1645 [24:44<2:59:33,  7.43s/it, train_loss=1.1562]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Menyimpan checkpoint di Step 15000...\n",
      "Checkpoint disimpan ke /home/ubuntu/skripsi/indonesia/gemma_adapter_mlp_checkpoint.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10:  42%|████▏     | 695/1645 [1:29:10<2:02:15,  7.72s/it, train_loss=1.1562]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Menyimpan checkpoint di Step 15500...\n",
      "Checkpoint disimpan ke /home/ubuntu/skripsi/indonesia/gemma_adapter_mlp_checkpoint.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10:  73%|███████▎  | 1195/1645 [2:34:47<55:33,  7.41s/it, train_loss=1.0312]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Menyimpan checkpoint di Step 16000...\n",
      "Checkpoint disimpan ke /home/ubuntu/skripsi/indonesia/gemma_adapter_mlp_checkpoint.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10: 100%|██████████| 1645/1645 [3:33:10<00:00,  7.78s/it, train_loss=1.1719]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Rata-rata Train Loss: 1.2722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validasi Epoch 10: 100%|██████████| 205/205 [05:58<00:00,  1.75s/it, val_loss=2.6250]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Rata-rata Validation Loss: 2.4562\n",
      "Validation Loss tidak membaik.\n",
      "\n",
      "--- Training Selesai ---\n",
      "Model terbaik disimpan di: /home/ubuntu/skripsi/indonesia/gemma_adapter_mlp_terbaik.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoProcessor, CLIPVisionModel\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# === BARU: Matikan peringatan Tokenizer Parallelism ===\n",
    "# Ini untuk membungkam error: \"huggingface/tokenizers: The current process just got forked...\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "# ====================================================\n",
    "\n",
    "\n",
    "# --- 1. Konfigurasi dan Hyperparameter ---\n",
    "\n",
    "# Path dari data Anda\n",
    "IMAGES_FILE_PATH = \"/home/ubuntu/skripsi/indonesia/original\"\n",
    "TRAIN_CAPTIONS_PATH = \"/home/ubuntu/skripsi/indonesia/train_captions.csv\"\n",
    "VAL_CAPTIONS_PATH = \"/home/ubuntu/skripsi/indonesia/val_captions.csv\"\n",
    "\n",
    "# Path untuk model validasi TERBAIK\n",
    "BEST_MODEL_PATH = \"/home/ubuntu/skripsi/indonesia/gemma_adapter_mlp_terbaik.pt\" \n",
    "# Path untuk checkpoint LATIHAN TERAKHIR (untuk resume)\n",
    "CHECKPOINT_PATH = \"/home/ubuntu/skripsi/indonesia/gemma_adapter_mlp_checkpoint.pt\" \n",
    "\n",
    "# Hyperparameter\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_EPOCHS = 10 \n",
    "RANDOM_SEED = 42\n",
    "SAVE_EVERY_STEPS = 500 # Simpan setiap 500 batch\n",
    "\n",
    "# Setup device\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available() :\n",
    "  device = 'cuda'\n",
    "print(f\"Menggunakan device: {device}\")\n",
    "\n",
    "# --- 2. Arsitektur Model (Dengan Adapter yang Diperkuat) ---\n",
    "\n",
    "print(\"Mendefinisikan arsitektur model...\")\n",
    "\n",
    "# === Adapter yang Diperkuat (MLP 2-Layer) ===\n",
    "class MyAdaptor(nn.Module) :\n",
    "  def __init__(self, vis_token_embedding_size, word_embedding_size) :\n",
    "    super(MyAdaptor, self).__init__()\n",
    "    self.vis_token_embedding_size = vis_token_embedding_size\n",
    "    self.word_embedding_size = word_embedding_size\n",
    "    self.adapter_mlp = nn.Sequential(\n",
    "        nn.Linear(self.vis_token_embedding_size, self.word_embedding_size),\n",
    "        nn.GELU(), # Aktivasi non-linear\n",
    "        nn.Linear(self.word_embedding_size, self.word_embedding_size)\n",
    "    )\n",
    "  def forward(self, img_output) :\n",
    "    img_embed = self.adapter_mlp(img_output)\n",
    "    return img_embed\n",
    "# =======================================================\n",
    "\n",
    "# === FUNGSI: Custom Collate Function ===\n",
    "def my_collate_fn(batch):\n",
    "    images = [item[0] for item in batch]\n",
    "    captions = [item[1] for item in batch]\n",
    "    return images, captions\n",
    "# ==========================================\n",
    "\n",
    "\n",
    "# === Model Utama (Sama Persis Seperti Kode Referensi Anda) ===\n",
    "class MyModel(nn.Module) :\n",
    "  def __init__(self) :\n",
    "    super(MyModel, self).__init__()\n",
    "    self.model_language = AutoModelForCausalLM.from_pretrained(\n",
    "        \"google/gemma-2-2b-it\", \n",
    "        torch_dtype=torch.bfloat16, \n",
    "        device_map=\"auto\" # Gunakan accelerate\n",
    "    )\n",
    "    self.tokenizer_language = AutoTokenizer.from_pretrained(\"google/gemma-2-2b-it\", padding_side= 'right')\n",
    "    self.image_processor = AutoProcessor.from_pretrained(\"openai/clip-vit-base-patch32\").image_processor\n",
    "    self.model_image = CLIPVisionModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "\n",
    "    self.word_embedding_size = self.model_language.config.hidden_size \n",
    "    self.num_vocab = self.model_language.config.vocab_size \n",
    "    self.trigger_str_img = \"<start_image>\"\n",
    "    self.num_vis_token_summary = 50\n",
    "    self.vis_token_embedding_size = self.model_image.config.hidden_size \n",
    "    self.adaptor = MyAdaptor(self.vis_token_embedding_size,self.word_embedding_size )\n",
    "    self.dummy_img_token = (\" \".join([\"the\"]*self.num_vis_token_summary)).strip()\n",
    "\n",
    "  def search_trigger_idx(self, text_token, trigger_str) :\n",
    "    all_token = text_token\n",
    "    all_string_now = \"\"\n",
    "    all_token_now = []\n",
    "    dummy_start_token = None\n",
    "    for token_idx in range(len(all_token)) :\n",
    "      token_now = int(all_token[token_idx].detach().cpu().numpy())\n",
    "      all_token_now.append(token_now)\n",
    "      token_as_string = self.tokenizer_language.batch_decode([all_token_now],skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "      if trigger_str in token_as_string :\n",
    "        dummy_start_token = token_idx + 1\n",
    "        break\n",
    "    return dummy_start_token\n",
    "\n",
    "  def get_image_embed(self, image_input) :\n",
    "    image_input_float = image_input.to(device, dtype=self.model_image.dtype)\n",
    "    img_output = self.model_image(image_input_float)['last_hidden_state']\n",
    "    img_output_bfloat16 = img_output.to(torch.bfloat16)\n",
    "    img_embed = self.adaptor(img_output_bfloat16)\n",
    "    return img_embed\n",
    "\n",
    "  def split_and_replace(self, now_input_tokens, replacement_embed, start_loc) :\n",
    "    num_token = len(replacement_embed)\n",
    "    start_embed = now_input_tokens[0:start_loc]\n",
    "    end_embed = now_input_tokens[start_loc+num_token:]\n",
    "    replaced_embed = torch.cat((start_embed, replacement_embed.to(now_input_tokens.dtype), end_embed),0)\n",
    "    return replaced_embed\n",
    "\n",
    "  def forward_loss(self, image_input_raw, caption_output_raw) :\n",
    "    instruction_now =  \"<start_of_turn>user\\n\"\n",
    "    instruction_now += f\"<start_image> {self.dummy_img_token}\\n<end_image>\\n\"\n",
    "    instruction_now += f\"Create a simple description of the image!\\n<end_of_turn>\\n<start_of_turn>model\\n\"\n",
    "\n",
    "    image_input = self.image_processor(image_input_raw, return_tensors=\"pt\")['pixel_values']\n",
    "    image_input = image_input.to(device, dtype=self.model_image.dtype) \n",
    "\n",
    "    caption_output = self.tokenizer_language(caption_output_raw,padding=True,return_tensors=\"pt\")\n",
    "    caption_output['input_ids'] = caption_output['input_ids'].to(device)\n",
    "    caption_output['attention_mask'] = caption_output['attention_mask'].to(device)\n",
    "\n",
    "    img_embed = self.get_image_embed(image_input)\n",
    "\n",
    "    all_text_with_prompt = [instruction_now + temp_text for temp_text in self.tokenizer_language.batch_decode(caption_output['input_ids'], skip_special_tokens=True)]\n",
    "    all_tokens_with_prompt = self.tokenizer_language(all_text_with_prompt, padding=True, return_tensors=\"pt\")\n",
    "    all_tokens_with_prompt['input_ids'] = all_tokens_with_prompt['input_ids'].to(device).detach()\n",
    "    all_tokens_with_prompt['attention_mask'] = all_tokens_with_prompt['attention_mask'].to(device).detach()\n",
    "\n",
    "    all_token_prompt_embed = self.model_language.model.embed_tokens(all_tokens_with_prompt['input_ids'])\n",
    "    prompt_len = len(self.tokenizer_language([instruction_now])['input_ids'][0])\n",
    "    \n",
    "    caption_label_now = all_tokens_with_prompt['input_ids'][:,prompt_len:]\n",
    "    caption_label_now = F.one_hot(caption_label_now, self.num_vocab).to(device, dtype=torch.bfloat16)\n",
    "    attn_mask_now = all_tokens_with_prompt['attention_mask'][:,prompt_len:]\n",
    "\n",
    "    all_replaced_feature = []\n",
    "    for temp_idx in range(len(all_tokens_with_prompt['input_ids'])) :\n",
    "      tokens_text_now = all_tokens_with_prompt['input_ids'][temp_idx].detach().cpu()\n",
    "      dummy_location_caption = self.search_trigger_idx(tokens_text_now, self.trigger_str_img )\n",
    "      image_replaced_prompt = self.split_and_replace(all_token_prompt_embed[temp_idx], img_embed[temp_idx], dummy_location_caption)\n",
    "      all_replaced_feature.append(image_replaced_prompt)\n",
    "    \n",
    "    all_replaced_feature = torch.stack(all_replaced_feature)\n",
    "    logits_now = self.model_language(inputs_embeds =all_replaced_feature, attention_mask=all_tokens_with_prompt['attention_mask'])\n",
    "    logits_now = logits_now['logits']\n",
    "    caption_prediction_now = logits_now[:,prompt_len-1:-1]\n",
    "    caption_prediction_now = torch.softmax(caption_prediction_now,-1)\n",
    "    caption_prediction_now = torch.clamp(caption_prediction_now, min=1e-10, max=1.0 - 1e-10)\n",
    "\n",
    "    loss_lm = -torch.sum(caption_label_now*torch.log(caption_prediction_now),-1)\n",
    "    loss_lm = torch.sum(loss_lm*attn_mask_now,-1)/torch.sum(attn_mask_now,-1)\n",
    "    loss_lm = torch.mean(loss_lm)\n",
    "    return loss_lm\n",
    "\n",
    "# --- 3. PyTorch Dataset Loader (Cara Baru Memuat Data) ---\n",
    "\n",
    "print(\"Mendefinisikan Dataset Loader...\")\n",
    "\n",
    "class CaptionDataset(Dataset):\n",
    "    def __init__(self, captions_csv_path, images_dir):\n",
    "        self.df = pd.read_csv(captions_csv_path, sep='|')\n",
    "        self.images_dir = images_dir\n",
    "        if 'Image_name' not in self.df.columns or 'caption' not in self.df.columns:\n",
    "            raise ValueError(\"CSV harus memiliki kolom 'Image_name' dan 'caption'\")\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        image_name = self.df.iloc[idx]['Image_name']\n",
    "        caption = self.df.iloc[idx]['caption']\n",
    "        image_path = os.path.join(self.images_dir, image_name)\n",
    "        try:\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "        except FileNotFoundError:\n",
    "            print(f\"WARNING: File gambar tidak ditemukan di {image_path}. Mengambil data pertama sebagai fallback.\")\n",
    "            image_name_fallback = self.df.iloc[0]['Image_name']\n",
    "            caption = self.df.iloc[0]['caption']\n",
    "            image_path = os.path.join(self.images_dir, image_name_fallback)\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "        # === TAMBAHAN: Error handling untuk gambar korup ===\n",
    "        except Exception as e:\n",
    "            print(f\"WARNING: Gagal memuat gambar {image_path} (Error: {e}). Mengambil data pertama sebagai fallback.\")\n",
    "            image_name_fallback = self.df.iloc[0]['Image_name']\n",
    "            caption = self.df.iloc[0]['caption']\n",
    "            image_path = os.path.join(self.images_dir, image_name_fallback)\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "        # ===============================================\n",
    "        return image, caption\n",
    "\n",
    "# --- 4. Inisialisasi Model, Data, dan Optimizer ---\n",
    "\n",
    "print(\"Inisialisasi model, data loader, dan optimizer...\")\n",
    "model = MyModel()\n",
    "model.adaptor.to(device, dtype=torch.bfloat16)\n",
    "\n",
    "# Freeze LLM dan Vision Model\n",
    "for param in model.model_language.parameters() : param.requires_grad = False\n",
    "for param in model.model_image.parameters() : param.requires_grad = False\n",
    "for param in model.adaptor.parameters() : param.requires_grad = True\n",
    "\n",
    "optim = torch.optim.Adam(model.adaptor.parameters(), lr=LEARNING_RATE)\n",
    "train_dataset = CaptionDataset(captions_csv_path=TRAIN_CAPTIONS_PATH, images_dir=IMAGES_FILE_PATH)\n",
    "val_dataset = CaptionDataset(captions_csv_path=VAL_CAPTIONS_PATH, images_dir=IMAGES_FILE_PATH)\n",
    "\n",
    "# === DIUBAH: num_workers=2 dipertahankan untuk menguji perbaikan shared memory ===\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, collate_fn=my_collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, collate_fn=my_collate_fn)\n",
    "# ==============================================================================\n",
    "\n",
    "print(f\"Data Train: {len(train_dataset)} caption (loader: {len(train_loader)} batch)\")\n",
    "print(f\"Data Val:   {len(val_dataset)} caption (loader: {len(val_loader)} batch)\")\n",
    "\n",
    "# === Logika Load Checkpoint ===\n",
    "start_epoch = 0\n",
    "global_step = 0\n",
    "best_val_loss = float('inf') # Default jika tidak ada checkpoint\n",
    "\n",
    "if os.path.exists(CHECKPOINT_PATH):\n",
    "    print(f\"Mendeteksi checkpoint di {CHECKPOINT_PATH}. Memuat...\")\n",
    "    try:\n",
    "        checkpoint = torch.load(CHECKPOINT_PATH, map_location=device)\n",
    "        model.adaptor.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optim.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        global_step = checkpoint['global_step']\n",
    "        best_val_loss = checkpoint['best_val_loss']\n",
    "        print(f\"Checkpoint dimuat. Melanjutkan dari Epoch {start_epoch + 1}, Step {global_step}\")\n",
    "        \n",
    "        # Jika checkpoint disimpan tepat di akhir epoch, kita mulai epoch BERIKUTNYA\n",
    "        if global_step > 0 and global_step % len(train_loader) == 0:\n",
    "            print(\"Checkpoint terdeteksi di akhir epoch, memulai epoch berikutnya.\")\n",
    "            start_epoch += 1 \n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error memuat checkpoint: {e}. Memulai dari awal.\")\n",
    "        start_epoch = 0\n",
    "        global_step = 0\n",
    "        best_val_loss = float('inf')\n",
    "else:\n",
    "    print(\"Tidak ada checkpoint. Memulai training dari awal.\")\n",
    "# ==================================\n",
    "\n",
    "# --- 5. Training Loop ---\n",
    "\n",
    "print(\"--- Memulai Training ---\")\n",
    "\n",
    "for epoch in range(start_epoch, NUM_EPOCHS):\n",
    "    print(f\"\\n--- Epoch {epoch + 1}/{NUM_EPOCHS} ---\")\n",
    "    \n",
    "    # --- Training Phase ---\n",
    "    model.train() \n",
    "    total_train_loss = 0\n",
    "    \n",
    "    # === PERBAIKAN: Pindahkan print 'Melanjutkan' ke sini ===\n",
    "    # Dicetak sebelum progress bar dibuat agar tidak merusak tampilan\n",
    "    if epoch == start_epoch and global_step % len(train_loader) != 0:\n",
    "        resume_step = global_step % len(train_loader)\n",
    "        print(f\"Melanjutkan epoch {epoch + 1} dari step {resume_step}\")\n",
    "    # ========================================================\n",
    "    \n",
    "    train_pbar = tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}\")\n",
    "    \n",
    "    # === PERBAIKAN: Blok .update() tqdm DIHAPUS ===\n",
    "    # Blok 'if... train_pbar.update(resume_step)' dihapus\n",
    "    # karena menyebabkan progress bar bingung.\n",
    "    # Logika 'continue' di bawah sudah cukup.\n",
    "    # ===============================================\n",
    "\n",
    "    for i, (images, captions) in enumerate(train_pbar):\n",
    "        \n",
    "        # === Logika Skip (SUDAH BENAR) ===\n",
    "        # Melewati batch yang sudah diproses di epoch ini\n",
    "        if epoch == start_epoch and i < (global_step % len(train_loader)):\n",
    "            continue\n",
    "        # ==================================\n",
    "        \n",
    "        loss = model.forward_loss(images, captions)\n",
    "        \n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        total_train_loss += loss.item()\n",
    "        global_step += 1 \n",
    "        \n",
    "        train_pbar.set_postfix({'train_loss': f\"{loss.item():.4f}\"})\n",
    "\n",
    "        # === Simpan checkpoint setiap 500 step ===\n",
    "        if global_step % SAVE_EVERY_STEPS == 0:\n",
    "            print(f\"\\nMenyimpan checkpoint di Step {global_step}...\")\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'global_step': global_step,\n",
    "                'model_state_dict': model.adaptor.state_dict(),\n",
    "                'optimizer_state_dict': optim.state_dict(),\n",
    "                'best_val_loss': best_val_loss,\n",
    "            }, CHECKPOINT_PATH)\n",
    "            print(f\"Checkpoint disimpan ke {CHECKPOINT_PATH}\")\n",
    "        # ============================================\n",
    "\n",
    "    # === PERBAIKAN: Kalkulasi Rata-rata Loss ===\n",
    "    # Logika ini menghitung jumlah batch yang *benar-benar* diproses\n",
    "    processed_batches_in_epoch = len(train_loader)\n",
    "    if epoch == start_epoch:\n",
    "        # Kurangi dengan batch yang di-skip\n",
    "        processed_batches_in_epoch -= (global_step % len(train_loader)) \n",
    "\n",
    "    # Hindari pembagian dengan nol jika epoch selesai tepat di akhir\n",
    "    if processed_batches_in_epoch > 0:\n",
    "        avg_train_loss = total_train_loss / processed_batches_in_epoch\n",
    "        print(f\"Epoch {epoch + 1} Rata-rata Train Loss: {avg_train_loss:.4f}\")\n",
    "    else:\n",
    "        print(f\"Epoch {epoch + 1} Selesai (melanjutkan dari checkpoint).\")\n",
    "    # ==========================================\n",
    "\n",
    "    # --- Validation Phase ---\n",
    "    model.eval() \n",
    "    total_val_loss = 0\n",
    "    \n",
    "    val_pbar = tqdm(val_loader, desc=f\"Validasi Epoch {epoch + 1}\")\n",
    "    with torch.no_grad(): \n",
    "        for images, captions in val_pbar:\n",
    "            loss = model.forward_loss(images, captions)\n",
    "            total_val_loss += loss.item()\n",
    "            val_pbar.set_postfix({'val_loss': f\"{loss.item():.4f}\"})\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    print(f\"Epoch {epoch + 1} Rata-rata Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # --- Save Best Model ---\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        print(f\"Validation Loss membaik! Menyimpan model terbaik ke {BEST_MODEL_PATH}\")\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'global_step': global_step,\n",
    "            'model_state_dict': model.adaptor.state_dict(),\n",
    "            'optimizer_state_dict': optim.state_dict(),\n",
    "            'best_val_loss': best_val_loss,\n",
    "        }, BEST_MODEL_PATH)\n",
    "    else:\n",
    "        print(\"Validation Loss tidak membaik.\")\n",
    "\n",
    "print(\"\\n--- Training Selesai ---\")\n",
    "print(f\"Model terbaik disimpan di: {BEST_MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3ebaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "/home/ubuntu/anaconda3/envs/skripsi_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
    "  from .autonotebook import tqdm as notebook_tqdm\n",
    "Menggunakan device: cuda\n",
    "Mendefinisikan arsitektur model...\n",
    "Mendefinisikan Dataset Loader...\n",
    "Inisialisasi model, data loader, dan optimizer...\n",
    "`torch_dtype` is deprecated! Use `dtype` instead!\n",
    "Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.54s/it]\n",
    "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
    "Data Train: 13154 caption (loader: 1645 batch)\n",
    "Data Val:   1639 caption (loader: 205 batch)\n",
    "Mendeteksi checkpoint di /home/ubuntu/skripsi/indonesia/gemma_adapter_mlp_checkpoint.pt. Memuat...\n",
    "Checkpoint dimuat. Melanjutkan dari Epoch 1, Step 1000\n",
    "--- Memulai Training ---\n",
    "\n",
    "--- Epoch 1/10 ---\n",
    "Melanjutkan epoch 1 dari step 1000\n",
    "Training Epoch 1:  91%|█████████ | 1499/1645 [1:04:11<32:12, 13.23s/it, train_loss=3.5781]\n",
    "\n",
    "Menyimpan checkpoint di Step 1500...\n",
    "Training Epoch 1:  91%|█████████ | 1500/1645 [1:04:11<32:42, 13.53s/it, train_loss=3.5781]\n",
    "Checkpoint disimpan ke /home/ubuntu/skripsi/indonesia/gemma_adapter_mlp_checkpoint.pt\n",
    "Training Epoch 1: 100%|██████████| 1645/1645 [1:35:55<00:00,  3.50s/it, train_loss=1.7969]\n",
    "Epoch 1 Rata-rata Train Loss: 0.9305\n",
    "Validasi Epoch 1: 100%|██████████| 205/205 [20:50<00:00,  6.10s/it, val_loss=2.4531]\n",
    "Epoch 1 Rata-rata Validation Loss: 2.4104\n",
    "Validation Loss membaik! Menyimpan model terbaik ke /home/ubuntu/skripsi/indonesia/gemma_adapter_mlp_terbaik.pt\n",
    "\n",
    "--- Epoch 2/10 ---\n",
    "Training Epoch 2:  22%|██▏       | 355/1645 [1:19:01<4:45:23, 13.27s/it, train_loss=2.9062]\n",
    "\n",
    "Menyimpan checkpoint di Step 2000...\n",
    "Checkpoint disimpan ke /home/ubuntu/skripsi/indonesia/gemma_adapter_mlp_checkpoint.pt\n",
    "Training Epoch 2:  52%|█████▏    | 854/1645 [3:08:54<2:56:24, 13.38s/it, train_loss=2.2500]\n",
    "\n",
    "Menyimpan checkpoint di Step 2500...\n",
    "Training Epoch 2:  52%|█████▏    | 855/1645 [3:08:54<2:57:24, 13.47s/it, train_loss=2.2500]\n",
    "Checkpoint disimpan ke /home/ubuntu/skripsi/indonesia/gemma_adapter_mlp_checkpoint.pt\n",
    "Training Epoch 2:  82%|████████▏ | 1355/1645 [4:59:52<1:01:20, 12.69s/it, train_loss=1.8984]\n",
    "\n",
    "Menyimpan checkpoint di Step 3000...\n",
    "Checkpoint disimpan ke /home/ubuntu/skripsi/indonesia/gemma_adapter_mlp_checkpoint.pt\n",
    "Training Epoch 2: 100%|██████████| 1645/1645 [6:03:35<00:00, 13.26s/it, train_loss=1.8828]  \n",
    "Epoch 2 Rata-rata Train Loss: 2.1994\n",
    "Validasi Epoch 2: 100%|██████████| 205/205 [21:23<00:00,  6.26s/it, val_loss=2.4531]\n",
    "Epoch 2 Rata-rata Validation Loss: 2.2826\n",
    "Validation Loss membaik! Menyimpan model terbaik ke /home/ubuntu/skripsi/indonesia/gemma_adapter_mlp_terbaik.pt\n",
    "\n",
    "--- Epoch 3/10 ---\n",
    "Training Epoch 3:  13%|█▎        | 210/1645 [45:50<5:11:31, 13.03s/it, train_loss=2.2500]\n",
    "\n",
    "Menyimpan checkpoint di Step 3500...\n",
    "Checkpoint disimpan ke /home/ubuntu/skripsi/indonesia/gemma_adapter_mlp_checkpoint.pt\n",
    "Training Epoch 3:  43%|████▎     | 710/1645 [2:36:33<3:39:04, 14.06s/it, train_loss=1.6328]\n",
    "\n",
    "Menyimpan checkpoint di Step 4000...\n",
    "Checkpoint disimpan ke /home/ubuntu/skripsi/indonesia/gemma_adapter_mlp_checkpoint.pt\n",
    "Training Epoch 3:  74%|███████▎  | 1210/1645 [4:28:19<1:36:20, 13.29s/it, train_loss=2.0156]\n",
    "\n",
    "Menyimpan checkpoint di Step 4500...\n",
    "Checkpoint disimpan ke /home/ubuntu/skripsi/indonesia/gemma_adapter_mlp_checkpoint.pt\n",
    "Training Epoch 3: 100%|██████████| 1645/1645 [6:03:10<00:00, 13.25s/it, train_loss=1.7734]  \n",
    "Epoch 3 Rata-rata Train Loss: 2.0191\n",
    "Validasi Epoch 3: 100%|██████████| 205/205 [21:16<00:00,  6.23s/it, val_loss=2.5000]\n",
    "Epoch 3 Rata-rata Validation Loss: 2.2517\n",
    "Validation Loss membaik! Menyimpan model terbaik ke /home/ubuntu/skripsi/indonesia/gemma_adapter_mlp_terbaik.pt\n",
    "\n",
    "--- Epoch 4/10 ---\n",
    "Training Epoch 4:   4%|▍         | 65/1645 [14:19<6:20:56, 14.47s/it, train_loss=1.8359]\n",
    "\n",
    "Menyimpan checkpoint di Step 5000...\n",
    "Checkpoint disimpan ke /home/ubuntu/skripsi/indonesia/gemma_adapter_mlp_checkpoint.pt\n",
    "Training Epoch 4:  34%|███▍      | 564/1645 [2:05:02<3:53:20, 12.95s/it, train_loss=2.3750]\n",
    "\n",
    "Menyimpan checkpoint di Step 5500...\n",
    "Training Epoch 4:  34%|███▍      | 565/1645 [2:05:02<3:57:07, 13.17s/it, train_loss=2.3750]\n",
    "Checkpoint disimpan ke /home/ubuntu/skripsi/indonesia/gemma_adapter_mlp_checkpoint.pt\n",
    "Training Epoch 4:  65%|██████▍   | 1065/1645 [3:55:54<2:15:31, 14.02s/it, train_loss=1.7969]\n",
    "\n",
    "Menyimpan checkpoint di Step 6000...\n",
    "Checkpoint disimpan ke /home/ubuntu/skripsi/indonesia/gemma_adapter_mlp_checkpoint.pt\n",
    "Training Epoch 4:  95%|█████████▌| 1564/1645 [5:47:45<19:10, 14.21s/it, train_loss=2.1875]  \n",
    "\n",
    "Menyimpan checkpoint di Step 6500...\n",
    "Training Epoch 4:  95%|█████████▌| 1565/1645 [5:47:45<18:28, 13.86s/it, train_loss=2.1875]\n",
    "Checkpoint disimpan ke /home/ubuntu/skripsi/indonesia/gemma_adapter_mlp_checkpoint.pt\n",
    "Training Epoch 4: 100%|██████████| 1645/1645 [6:05:16<00:00, 13.32s/it, train_loss=2.4219]\n",
    "Epoch 4 Rata-rata Train Loss: 1.8755\n",
    "Validasi Epoch 4: 100%|██████████| 205/205 [21:25<00:00,  6.27s/it, val_loss=2.5469]\n",
    "Epoch 4 Rata-rata Validation Loss: 2.2392\n",
    "Validation Loss membaik! Menyimpan model terbaik ke /home/ubuntu/skripsi/indonesia/gemma_adapter_mlp_terbaik.pt\n",
    "\n",
    "--- Epoch 5/10 ---\n",
    "Training Epoch 5:  26%|██▌       | 420/1645 [1:33:02<4:20:29, 12.76s/it, train_loss=1.9844]\n",
    "\n",
    "Menyimpan checkpoint di Step 7000...\n",
    "Checkpoint disimpan ke /home/ubuntu/skripsi/indonesia/gemma_adapter_mlp_checkpoint.pt\n",
    "Training Epoch 5:  56%|█████▌    | 920/1645 [3:22:50<2:49:56, 14.06s/it, train_loss=1.9375]\n",
    "\n",
    "Menyimpan checkpoint di Step 7500...\n",
    "Checkpoint disimpan ke /home/ubuntu/skripsi/indonesia/gemma_adapter_mlp_checkpoint.pt\n",
    "Training Epoch 5:  86%|████████▋ | 1419/1645 [5:11:32<47:45, 12.68s/it, train_loss=1.7891]  \n",
    "\n",
    "Menyimpan checkpoint di Step 8000...\n",
    "Training Epoch 5:  86%|████████▋ | 1420/1645 [5:11:32<48:53, 13.04s/it, train_loss=1.7891]\n",
    "Checkpoint disimpan ke /home/ubuntu/skripsi/indonesia/gemma_adapter_mlp_checkpoint.pt\n",
    "Training Epoch 5: 100%|██████████| 1645/1645 [6:00:36<00:00, 13.15s/it, train_loss=2.7031]\n",
    "Epoch 5 Rata-rata Train Loss: 1.7486\n",
    "Validasi Epoch 5: 100%|██████████| 205/205 [20:52<00:00,  6.11s/it, val_loss=2.5938]\n",
    "Epoch 5 Rata-rata Validation Loss: 2.2587\n",
    "Validation Loss tidak membaik.\n",
    "\n",
    "--- Epoch 6/10 ---\n",
    "Training Epoch 6:  17%|█▋        | 274/1645 [1:01:17<5:19:14, 13.97s/it, train_loss=2.0938]\n",
    "\n",
    "Menyimpan checkpoint di Step 8500...\n",
    "Training Epoch 6:  17%|█▋        | 275/1645 [1:01:17<5:23:11, 14.15s/it, train_loss=2.0938]\n",
    "Checkpoint disimpan ke /home/ubuntu/skripsi/indonesia/gemma_adapter_mlp_checkpoint.pt\n",
    "Training Epoch 6:  47%|████▋     | 775/1645 [2:51:03<3:20:06, 13.80s/it, train_loss=1.7422]\n",
    "\n",
    "Menyimpan checkpoint di Step 9000...\n",
    "Checkpoint disimpan ke /home/ubuntu/skripsi/indonesia/gemma_adapter_mlp_checkpoint.pt\n",
    "Training Epoch 6:  78%|███████▊  | 1275/1645 [4:40:54<1:25:39, 13.89s/it, train_loss=1.9062]\n",
    "\n",
    "Menyimpan checkpoint di Step 9500...\n",
    "Checkpoint disimpan ke /home/ubuntu/skripsi/indonesia/gemma_adapter_mlp_checkpoint.pt\n",
    "Training Epoch 6: 100%|██████████| 1645/1645 [6:01:15<00:00, 13.18s/it, train_loss=2.3125]  \n",
    "Epoch 6 Rata-rata Train Loss: 1.6360\n",
    "Validasi Epoch 6: 100%|██████████| 205/205 [21:09<00:00,  6.19s/it, val_loss=2.6250]\n",
    "Epoch 6 Rata-rata Validation Loss: 2.2891\n",
    "Validation Loss tidak membaik.\n",
    "\n",
    "--- Epoch 7/10 ---\n",
    "Training Epoch 7:   8%|▊         | 130/1645 [28:31<5:55:56, 14.10s/it, train_loss=1.5625]\n",
    "\n",
    "Menyimpan checkpoint di Step 10000...\n",
    "Checkpoint disimpan ke /home/ubuntu/skripsi/indonesia/gemma_adapter_mlp_checkpoint.pt\n",
    "Training Epoch 7:  38%|███▊      | 629/1645 [2:18:31<3:50:39, 13.62s/it, train_loss=1.3594]\n",
    "\n",
    "Menyimpan checkpoint di Step 10500...\n",
    "Training Epoch 7:  38%|███▊      | 630/1645 [2:18:32<3:43:38, 13.22s/it, train_loss=1.3594]\n",
    "Checkpoint disimpan ke /home/ubuntu/skripsi/indonesia/gemma_adapter_mlp_checkpoint.pt\n",
    "Training Epoch 7:  40%|████      | 659/1645 [6:04:24<9:05:13, 33.18s/it, train_loss=1.4844]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa57d454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Menggunakan device: cuda\n",
      "Mendefinisikan arsitektur model (sesuai training)...\n",
      "Inisialisasi model (arsitektur training)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.86s/it]\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memuat checkpoint dari: /home/ubuntu/skripsi/indonesia/gemma_adapter_mlp_checkpoint.pt\n",
      "Berhasil memuat model dari Epoch 9, Step 16000\n",
      "Memuat data tes dari: /home/ubuntu/skripsi/indonesia/test_captions.csv\n",
      "Mengelompokkan referensi caption...\n",
      "Ditemukan 301 gambar unik di test set.\n",
      "Memulai evaluasi...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mengevaluasi Model: 100%|██████████| 301/301 [10:57<00:00,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Menghitung BLEU Score ---\n",
      "\n",
      "--- Contoh Hasil ---\n",
      "Gambar: F001.jpg\n",
      "  Prediksi: \n",
      "  Ref 1:    sepeda motor terparkir rapi di area parkir, sementara dua orang berjalan di latar belakang.\n",
      "Gambar: F008.jpg\n",
      "  Prediksi: \n",
      "  Ref 1:    naik ke pengalaman baru dengan setiap langkah di tangga ceria ini!\n",
      "Gambar: F031.jpg\n",
      "  Prediksi: \n",
      "  Ref 1:    terdapat wastafel putih di dinding, dengan tanda larangan dan beberapa bahan konstruksi di sekitar. suasana sederhana.\n",
      "Gambar: F044.jpg\n",
      "  Prediksi: \n",
      "  Ref 1:    area parkir yang dikelilingi pepohonan, dengan beberapa sepeda motor terparkir dan tanda selamat datang.\n",
      "Gambar: F046.jpg\n",
      "  Prediksi: \n",
      "  Ref 1:    jalur pejalan kaki dengan pegangan tangan, dikelilingi dinding dan suasana tenang, mengarah ke area terbuka.\n",
      "\n",
      "Total gambar dievaluasi: 301 / 301\n",
      "BLEU-1: 0.00\n",
      "BLEU-2: 0.00\n",
      "BLEU-3: 0.00\n",
      "BLEU-4 (Standar): 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, AutoTokenizer, AutoModelForCausalLM, CLIPVisionModel\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import torch.nn.functional as F\n",
    "import numpy as np # Ditambahkan\n",
    "\n",
    "# --- 1. KONFIGURASI PATH (Sama seperti evaluasi Anda) ---\n",
    "BASE_DIR = \"/home/ubuntu/skripsi/indonesia\"\n",
    "# PENTING: Muat checkpoint dari training Anda\n",
    "MODEL_PATH = os.path.join(BASE_DIR, \"gemma_adapter_mlp_checkpoint.pt\") \n",
    "TEST_DATA_PATH = os.path.join(BASE_DIR, \"test_captions.csv\")\n",
    "IMAGE_DIR = \"/home/ubuntu/skripsi/indonesia/original\" \n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Menggunakan device: {DEVICE}\")\n",
    "\n",
    "# --- 2. DEFINISI MODEL (HARUS IDENTIK DENGAN TRAINING CELL [7]) ---\n",
    "\n",
    "print(\"Mendefinisikan arsitektur model (sesuai training)...\")\n",
    "\n",
    "# === Adapter yang Dilatih (dari Cell 7) ===\n",
    "class MyAdaptor(nn.Module) :\n",
    "  def __init__(self, vis_token_embedding_size, word_embedding_size) :\n",
    "    super(MyAdaptor, self).__init__()\n",
    "    self.vis_token_embedding_size = vis_token_embedding_size\n",
    "    self.word_embedding_size = word_embedding_size\n",
    "    self.adapter_mlp = nn.Sequential(\n",
    "        nn.Linear(self.vis_token_embedding_size, self.word_embedding_size),\n",
    "        nn.GELU(), # Aktivasi non-linear\n",
    "        nn.Linear(self.word_embedding_size, self.word_embedding_size)\n",
    "    )\n",
    "  def forward(self, img_output) :\n",
    "    img_embed = self.adapter_mlp(img_output)\n",
    "    return img_embed\n",
    "\n",
    "# === Model Utama (dari Cell 7) ===\n",
    "class MyModel(nn.Module) :\n",
    "  def __init__(self) :\n",
    "    super(MyModel, self).__init__()\n",
    "    # Inisialisasi model persis seperti di Cell 7\n",
    "    self.model_language = AutoModelForCausalLM.from_pretrained(\n",
    "        \"google/gemma-2-2b-it\", \n",
    "        torch_dtype=torch.bfloat16, \n",
    "        device_map=\"auto\" # Gunakan accelerate\n",
    "    )\n",
    "    self.tokenizer_language = AutoTokenizer.from_pretrained(\"google/gemma-2-2b-it\", padding_side= 'right')\n",
    "    self.image_processor = AutoProcessor.from_pretrained(\"openai/clip-vit-base-patch32\").image_processor\n",
    "    # Pindahkan CLIP model ke 'device' (cuda)\n",
    "    self.model_image = CLIPVisionModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(DEVICE)\n",
    "\n",
    "    self.word_embedding_size = self.model_language.config.hidden_size \n",
    "    self.num_vocab = self.model_language.config.vocab_size \n",
    "    self.trigger_str_img = \"<start_image>\"\n",
    "    self.num_vis_token_summary = 50\n",
    "    self.vis_token_embedding_size = self.model_image.config.hidden_size \n",
    "    # Gunakan MyAdaptor yang benar\n",
    "    self.adaptor = MyAdaptor(self.vis_token_embedding_size,self.word_embedding_size )\n",
    "    self.dummy_img_token = (\" \".join([\"the\"]*self.num_vis_token_summary)).strip()\n",
    "\n",
    "  # Fungsi 'search_trigger_idx' (dari Cell 7)\n",
    "  def search_trigger_idx(self, text_token, trigger_str) :\n",
    "    all_token = text_token\n",
    "    all_string_now = \"\"\n",
    "    all_token_now = []\n",
    "    dummy_start_token = None\n",
    "    for token_idx in range(len(all_token)) :\n",
    "      token_now = int(all_token[token_idx].detach().cpu().numpy())\n",
    "      all_token_now.append(token_now)\n",
    "      token_as_string = self.tokenizer_language.batch_decode([all_token_now],skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "      if trigger_str in token_as_string :\n",
    "        dummy_start_token = token_idx + 1\n",
    "        break\n",
    "    return dummy_start_token\n",
    "\n",
    "  # Fungsi 'get_image_embed' (dari Cell 7)\n",
    "  def get_image_embed(self, image_input) :\n",
    "    # Pastikan input gambar ada di 'device' (cuda)\n",
    "    image_input_float = image_input.to(DEVICE, dtype=self.model_image.dtype)\n",
    "    img_output = self.model_image(image_input_float)['last_hidden_state']\n",
    "    img_output_bfloat16 = img_output.to(torch.bfloat16)\n",
    "    img_embed = self.adaptor(img_output_bfloat16)\n",
    "    return img_embed\n",
    "\n",
    "  # Fungsi 'split_and_replace' (dari Cell 7)\n",
    "  def split_and_replace(self, now_input_tokens, replacement_embed, start_loc) :\n",
    "    num_token = len(replacement_embed)\n",
    "    start_embed = now_input_tokens[0:start_loc]\n",
    "    end_embed = now_input_tokens[start_loc+num_token:]\n",
    "    replaced_embed = torch.cat((start_embed, replacement_embed.to(now_input_tokens.dtype), end_embed),0)\n",
    "    return replaced_embed\n",
    "\n",
    "  # === FUNGSI GENERATE BARU (Menggunakan logika training) ===\n",
    "  def generate_aswer_image(self, pil_image, max_new_tokens=64):\n",
    "    # 1. Buat prompt (hardcoded seperti di training)\n",
    "    instruction_now = \"<start_of_turn>user\\n\"\n",
    "    instruction_now += f\"<start_image> {self.dummy_img_token}\\n<end_image>\\n\"\n",
    "    instruction_now += f\"Create a simple description of the image!\\n<end_of_turn>\\n<start_of_turn>model\\n\"\n",
    "    \n",
    "    # 2. Tokenisasi prompt\n",
    "    prompt_tokens = self.tokenizer_language([instruction_now], padding=False, return_tensors=\"pt\")\n",
    "    # Pindahkan token ke device LLM\n",
    "    prompt_tokens = {k: v.to(self.model_language.device) for k, v in prompt_tokens.items()}\n",
    "    \n",
    "    # 3. Dapatkan text embeddings\n",
    "    prompt_embeds = self.model_language.model.embed_tokens(prompt_tokens['input_ids'])\n",
    "    \n",
    "    # 4. Dapatkan image embeddings\n",
    "    image_input = self.image_processor([pil_image], return_tensors=\"pt\")['pixel_values']\n",
    "    # 'image_input' diproses di 'DEVICE' (cuda)\n",
    "    img_embed = self.get_image_embed(image_input) # Shape [1, 50, 2304]\n",
    "\n",
    "    # 5. Cari lokasi penggantian (logika dari training)\n",
    "    tokens_text_now = prompt_tokens['input_ids'][0].detach().cpu()\n",
    "    dummy_location = self.search_trigger_idx(tokens_text_now, self.trigger_str_img)\n",
    "\n",
    "    if dummy_location is None:\n",
    "        print(\"WARNING: Tidak bisa menemukan trigger string di prompt.\")\n",
    "        return \"\" # Gagal generate\n",
    "\n",
    "    # 6. Ganti embedding secara manual (logika dari training)\n",
    "    # (Menggunakan batch index [0] karena kita proses satu per satu)\n",
    "    replaced_embeds = self.split_and_replace(prompt_embeds[0], img_embed[0], dummy_location)\n",
    "    \n",
    "    # 7. Tambahkan dimensi batch kembali\n",
    "    replaced_embeds = replaced_embeds.unsqueeze(0) # Shape [1, SeqLen, HiddenSize]\n",
    "\n",
    "    # 8. Generate!\n",
    "    output_now = self.model_language.generate(\n",
    "        inputs_embeds=replaced_embeds,\n",
    "        attention_mask=prompt_tokens['attention_mask'],\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        num_beams=5,\n",
    "        do_sample=False,\n",
    "        pad_token_id=self.tokenizer_language.eos_token_id\n",
    "    )\n",
    "\n",
    "    # 9. Decode\n",
    "    output_string = self.tokenizer_language.batch_decode(\n",
    "        output_now, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "    )[0]\n",
    "    \n",
    "    # 10. Bersihkan output\n",
    "    parts = output_string.split(\"model\\n\")\n",
    "    if len(parts) > 1:\n",
    "        return parts[-1].strip()\n",
    "    else:\n",
    "        return \"\" # Gagal menemukan 'model' token\n",
    "\n",
    "# --- 3. Muat Model dan Checkpoint ---\n",
    "print(\"Inisialisasi model (arsitektur training)...\")\n",
    "model = MyModel()\n",
    "\n",
    "# Pindahkan adaptor ke 'device' (cuda) sebelum memuat state dict\n",
    "model.adaptor.to(DEVICE, dtype=torch.bfloat16)\n",
    "\n",
    "print(f\"Memuat checkpoint dari: {MODEL_PATH}\")\n",
    "checkpoint = torch.load(MODEL_PATH, map_location=DEVICE)\n",
    "\n",
    "# --- PERBAIKAN UTAMA: Muat state_dict ke 'model.adaptor' ---\n",
    "model.adaptor.load_state_dict(checkpoint['model_state_dict'])\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "print(f\"Berhasil memuat model dari Epoch {checkpoint.get('epoch', 'N/A')}, Step {checkpoint.get('global_step', 'N/A')}\")\n",
    "model.eval() # Set model ke mode evaluasi\n",
    "\n",
    "# --- 4. Muat Data Test (Sama seperti evaluasi Anda) ---\n",
    "print(f\"Memuat data tes dari: {TEST_DATA_PATH}\")\n",
    "df_test = pd.read_csv(TEST_DATA_PATH, sep='|')\n",
    "\n",
    "print(\"Mengelompokkan referensi caption...\")\n",
    "references_map = {}\n",
    "for img_name, caption in zip(df_test['Image_name'], df_test['caption']): \n",
    "    if img_name not in references_map:\n",
    "        references_map[img_name] = []\n",
    "    # Pastikan caption adalah string sebelum di-split\n",
    "    references_map[img_name].append(str(caption).lower().split()) \n",
    "    \n",
    "unique_test_images = list(references_map.keys())\n",
    "print(f\"Ditemukan {len(unique_test_images)} gambar unik di test set.\")\n",
    "\n",
    "# --- 5. Jalankan Evaluasi ---\n",
    "print(\"Memulai evaluasi...\")\n",
    "candidates = [] \n",
    "references_list = [] \n",
    "\n",
    "with torch.no_grad():\n",
    "    for img_name in tqdm(unique_test_images, desc=\"Mengevaluasi Model\"):\n",
    "        img_path = os.path.join(IMAGE_DIR, img_name)\n",
    "        \n",
    "        try:\n",
    "            image_raw = Image.open(img_path).convert(\"RGB\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File tidak ditemukan: {img_path}\")\n",
    "            continue\n",
    "            \n",
    "        # Panggil fungsi generate baru kita\n",
    "        generated_text = model.generate_aswer_image(\n",
    "            image_raw,\n",
    "            max_new_tokens=64\n",
    "        )\n",
    "        \n",
    "        candidates.append(generated_text.lower().split())\n",
    "        references_list.append(references_map[img_name])\n",
    "\n",
    "# --- 6. Hitung Skor BLEU ---\n",
    "print(\"\\n--- Menghitung BLEU Score ---\")\n",
    "\n",
    "if len(candidates) == 0:\n",
    "    print(\"Evaluasi gagal. Tidak ada kandidat yang dihasilkan.\")\n",
    "else:\n",
    "    print(\"\\n--- Contoh Hasil ---\")\n",
    "    for i in range(min(5, len(candidates))):\n",
    "        print(f\"Gambar: {unique_test_images[i]}\")\n",
    "        print(f\"  Prediksi: {' '.join(candidates[i])}\")\n",
    "        print(f\"  Ref 1:    {' '.join(references_list[i][0])}\")\n",
    "    \n",
    "    bleu1 = corpus_bleu(references_list, candidates, weights=(1.0, 0, 0, 0))\n",
    "    bleu2 = corpus_bleu(references_list, candidates, weights=(0.5, 0.5, 0, 0))\n",
    "    bleu3 = corpus_bleu(references_list, candidates, weights=(0.33, 0.33, 0.33, 0))\n",
    "    bleu4 = corpus_bleu(references_list, candidates, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "    \n",
    "    print(f\"\\nTotal gambar dievaluasi: {len(candidates)} / {len(unique_test_images)}\")\n",
    "    print(f\"BLEU-1: {bleu1*100:.2f}\")\n",
    "    print(f\"BLEU-2: {bleu2*100:.2f}\")\n",
    "    print(f\"BLEU-3: {bleu3*100:.2f}\")\n",
    "    print(f\"BLEU-4 (Standar): {bleu4*100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31c33286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Menggunakan device: cuda\n",
      "Mendefinisikan arsitektur model (sesuai training Cell [7])...\n",
      "Inisialisasi model (arsitektur training)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  7.00s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memuat checkpoint dari: /home/ubuntu/skripsi/indonesia/gemma_adapter_mlp_checkpoint.pt\n",
      "Berhasil memuat model dari Step 16000\n",
      "\n",
      "--- DEBUG: Dummy Token ---\n",
      "String 'dummy_img_token' (50 'the') di-tokenize menjadi: 50 token.\n",
      "Jumlah token visual yang diharapkan (num_vis_token_summary): 50\n",
      "INFO: Jumlah token dummy sudah benar (50).\n",
      "--------------------------\n",
      "\n",
      "--- Tes Sanity Check ---\n",
      "Memuat gambar: /home/ubuntu/skripsi/indonesia/original/F001.jpg\n",
      "\n",
      "--- HASIL GENERASI (DEBUG) ---\n",
      "Gambar: F001.jpg\n",
      "\n",
      "OUTPUT MENTAH dari model:\n",
      "---\n",
      "Beberapa sepeda motor terparkir di depan sementara pohon-pohon rimbun dan suasana tenang mengelilingi bangunan-bangunan modern. \n",
      "\n",
      "---\n",
      "\n",
      "HASIL PREDIKSI (setelah split 'model\\n'):\n",
      "---\n",
      "\n",
      "---\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, AutoTokenizer, AutoModelForCausalLM, CLIPVisionModel\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "# --- 1. Konfigurasi Path ---\n",
    "IMAGE_DIR = \"/home/ubuntu/skripsi/indonesia/original\"\n",
    "MODEL_PATH = \"/home/ubuntu/skripsi/indonesia/gemma_adapter_mlp_checkpoint.pt\" \n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Menggunakan device: {DEVICE}\")\n",
    "print(\"Mendefinisikan arsitektur model (sesuai training Cell [7])...\")\n",
    "\n",
    "# --- 2. DEFINISI MODEL (IDENTIK DENGAN TRAINING CELL [7]) ---\n",
    "\n",
    "# === Adapter yang Dilatih (dari Cell 7) ===\n",
    "class MyAdaptor(nn.Module) :\n",
    "  def __init__(self, vis_token_embedding_size, word_embedding_size) :\n",
    "    super(MyAdaptor, self).__init__()\n",
    "    self.vis_token_embedding_size = vis_token_embedding_size\n",
    "    self.word_embedding_size = word_embedding_size\n",
    "    self.adapter_mlp = nn.Sequential(\n",
    "        nn.Linear(self.vis_token_embedding_size, self.word_embedding_size),\n",
    "        nn.GELU(),\n",
    "        nn.Linear(self.word_embedding_size, self.word_embedding_size)\n",
    "    )\n",
    "  def forward(self, img_output) :\n",
    "    img_embed = self.adapter_mlp(img_output)\n",
    "    return img_embed\n",
    "\n",
    "# === Model Utama (dari Cell 7) ===\n",
    "class MyModel(nn.Module) :\n",
    "  def __init__(self) :\n",
    "    super(MyModel, self).__init__()\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        self.model_language = AutoModelForCausalLM.from_pretrained(\n",
    "            \"google/gemma-2-2b-it\", \n",
    "            torch_dtype=torch.bfloat16, \n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "    self.tokenizer_language = AutoTokenizer.from_pretrained(\"google/gemma-2-2b-it\", padding_side= 'right')\n",
    "    self.image_processor = AutoProcessor.from_pretrained(\"openai/clip-vit-base-patch32\").image_processor\n",
    "    self.model_image = CLIPVisionModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(DEVICE)\n",
    "\n",
    "    self.word_embedding_size = self.model_language.config.hidden_size \n",
    "    self.num_vocab = self.model_language.config.vocab_size \n",
    "    self.trigger_str_img = \"<start_image>\"\n",
    "    self.num_vis_token_summary = 50\n",
    "    self.vis_token_embedding_size = self.model_image.config.hidden_size \n",
    "    self.adaptor = MyAdaptor(self.vis_token_embedding_size,self.word_embedding_size )\n",
    "    self.dummy_img_token = (\" \".join([\"the\"]*self.num_vis_token_summary)).strip()\n",
    "\n",
    "  def search_trigger_idx(self, text_token, trigger_str) :\n",
    "    all_token = text_token\n",
    "    all_string_now = \"\"\n",
    "    all_token_now = []\n",
    "    dummy_start_token = None\n",
    "    for token_idx in range(len(all_token)) :\n",
    "      token_now = int(all_token[token_idx].detach().cpu().numpy())\n",
    "      all_token_now.append(token_now)\n",
    "      token_as_string = self.tokenizer_language.batch_decode([all_token_now],skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "      if trigger_str in token_as_string :\n",
    "        dummy_start_token = token_idx + 1\n",
    "        break\n",
    "    return dummy_start_token\n",
    "\n",
    "  def get_image_embed(self, image_input) :\n",
    "    image_input_float = image_input.to(DEVICE, dtype=self.model_image.dtype)\n",
    "    img_output = self.model_image(image_input_float)['last_hidden_state']\n",
    "    img_output_bfloat16 = img_output.to(torch.bfloat16)\n",
    "    img_embed = self.adaptor(img_output_bfloat16)\n",
    "    return img_embed\n",
    "\n",
    "  def split_and_replace(self, now_input_tokens, replacement_embed, start_loc) :\n",
    "    num_token = len(replacement_embed)\n",
    "    start_embed = now_input_tokens[0:start_loc]\n",
    "    end_embed = now_input_tokens[start_loc+num_token:]\n",
    "    replaced_embed = torch.cat((start_embed, replacement_embed.to(now_input_tokens.dtype), end_embed),0)\n",
    "    return replaced_embed\n",
    "\n",
    "  # --- FUNGSI GENERATE (Diperbarui untuk DEBUG) ---\n",
    "  def generate_aswer_image(self, pil_image, max_new_tokens=64):\n",
    "    instruction_now = \"<start_of_turn>user\\n\"\n",
    "    instruction_now += f\"<start_image> {self.dummy_img_token}\\n<end_image>\\n\"\n",
    "    instruction_now += f\"Create a simple description of the image!\\n<end_of_turn>\\n<start_of_turn>model\\n\"\n",
    "    \n",
    "    prompt_tokens = self.tokenizer_language([instruction_now], padding=False, return_tensors=\"pt\")\n",
    "    prompt_tokens = {k: v.to(self.model_language.device) for k, v in prompt_tokens.items()}\n",
    "    \n",
    "    prompt_embeds = self.model_language.model.embed_tokens(prompt_tokens['input_ids'])\n",
    "    \n",
    "    image_input = self.image_processor([pil_image], return_tensors=\"pt\")['pixel_values']\n",
    "    img_embed = self.get_image_embed(image_input) \n",
    "\n",
    "    tokens_text_now = prompt_tokens['input_ids'][0].detach().cpu()\n",
    "    dummy_location = self.search_trigger_idx(tokens_text_now, self.trigger_str_img)\n",
    "\n",
    "    if dummy_location is None:\n",
    "        print(\"WARNING: Tidak bisa menemukan trigger string di prompt.\")\n",
    "        return \"\", \"\" # Kembalikan string mentah juga\n",
    "\n",
    "    replaced_embeds = self.split_and_replace(prompt_embeds[0], img_embed[0], dummy_location)\n",
    "    replaced_embeds = replaced_embeds.unsqueeze(0) \n",
    "\n",
    "    output_now = self.model_language.generate(\n",
    "        inputs_embeds=replaced_embeds,\n",
    "        attention_mask=prompt_tokens['attention_mask'],\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        num_beams=5,\n",
    "        do_sample=False,\n",
    "        pad_token_id=self.tokenizer_language.eos_token_id\n",
    "    )\n",
    "\n",
    "    output_string = self.tokenizer_language.batch_decode(\n",
    "        output_now, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "    )[0]\n",
    "    \n",
    "    # Bersihkan output\n",
    "    parts = output_string.split(\"model\\n\")\n",
    "    generated_text = \"\"\n",
    "    if len(parts) > 1:\n",
    "        generated_text = parts[-1].strip()\n",
    "    \n",
    "    return output_string, generated_text # Kembalikan KEDUANYA\n",
    "\n",
    "# --- 3. Muat Model dan Checkpoint ---\n",
    "print(\"Inisialisasi model (arsitektur training)...\")\n",
    "model = MyModel()\n",
    "model.adaptor.to(DEVICE, dtype=torch.bfloat16)\n",
    "\n",
    "print(f\"Memuat checkpoint dari: {MODEL_PATH}\")\n",
    "checkpoint = torch.load(MODEL_PATH, map_location=DEVICE)\n",
    "model.adaptor.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(f\"Berhasil memuat model dari Step {checkpoint.get('global_step', 'N/A')}\")\n",
    "model.eval()\n",
    "\n",
    "# --- 4. DEBUG TOKENIZER ---\n",
    "dummy_token_string = (\" \".join([\"the\"]*50)).strip()\n",
    "dummy_tokens = model.tokenizer_language(dummy_token_string, add_special_tokens=False)['input_ids']\n",
    "print(f\"\\n--- DEBUG: Dummy Token ---\")\n",
    "print(f\"String 'dummy_img_token' (50 'the') di-tokenize menjadi: {len(dummy_tokens)} token.\")\n",
    "print(f\"Jumlah token visual yang diharapkan (num_vis_token_summary): 50\")\n",
    "if len(dummy_tokens) != 50:\n",
    "    print(\"WARNING: Jumlah token dummy TIDAK SAMA DENGAN 50! Ini bisa jadi sumber masalah.\")\n",
    "else:\n",
    "    print(\"INFO: Jumlah token dummy sudah benar (50).\")\n",
    "print(f\"--------------------------\")\n",
    "\n",
    "\n",
    "# --- 5. TES GAMBAR TUNGGAL ---\n",
    "IMAGE_NAME_TO_TEST = \"F001.jpg\"\n",
    "image_path_to_test = os.path.join(IMAGE_DIR, IMAGE_NAME_TO_TEST)\n",
    "\n",
    "print(f\"\\n--- Tes Sanity Check ---\")\n",
    "print(f\"Memuat gambar: {image_path_to_test}\")\n",
    "\n",
    "try:\n",
    "    image_raw = Image.open(image_path_to_test).convert(\"RGB\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        raw_output, generated_text = model.generate_aswer_image(\n",
    "            image_raw,\n",
    "            max_new_tokens=64\n",
    "        )\n",
    "    \n",
    "    print(\"\\n--- HASIL GENERASI (DEBUG) ---\")\n",
    "    print(f\"Gambar: {IMAGE_NAME_TO_TEST}\")\n",
    "    print(f\"\\nOUTPUT MENTAH dari model:\\n---\\n{raw_output}\\n---\")\n",
    "    print(f\"\\nHASIL PREDIKSI (setelah split 'model\\\\n'):\\n---\\n{generated_text}\\n---\")\n",
    "    print(\"---------------------------------\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: File gambar tidak ditemukan di {image_path_to_test}\")\n",
    "except Exception as e:\n",
    "    print(f\"Terjadi error saat generasi: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df0c1e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Menggunakan device: cuda\n",
      "Mendefinisikan arsitektur model (sesuai training)...\n",
      "Inisialisasi model (arsitektur training)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.10s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the disk and cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memuat checkpoint dari: /home/ubuntu/skripsi/indonesia/gemma_adapter_mlp_checkpoint.pt\n",
      "Berhasil memuat model dari Step 16000\n",
      "Memuat data tes dari: /home/ubuntu/skripsi/indonesia/test_captions.csv\n",
      "Mengelompokkan referensi caption...\n",
      "Ditemukan 301 gambar unik di test set.\n",
      "Memulai evaluasi...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mengevaluasi Model: 100%|██████████| 301/301 [18:15:46<00:00, 218.43s/it]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Menghitung BLEU Score ---\n",
      "\n",
      "--- Contoh Hasil ---\n",
      "Gambar: F001.jpg\n",
      "  Prediksi: beberapa sepeda motor terparkir di depan sementara pohon-pohon rimbun dan suasana tenang mengelilingi bangunan-bangunan modern.\n",
      "  Ref 1:    sepeda motor terparkir rapi di area parkir, sementara dua orang berjalan di latar belakang.\n",
      "Gambar: F008.jpg\n",
      "  Prediksi: di depan ada tangga berwarna kuning dengan anak tangga berwarna hijau dan kuning yang ada di dekatnya\n",
      "  Ref 1:    naik ke pengalaman baru dengan setiap langkah di tangga ceria ini!\n",
      "Gambar: F031.jpg\n",
      "  Prediksi: di depan terlihat wastafel keramik berwarna putih dan tembaga serta dua botol minuman berwarna biru dan merah berada di atas wastafel.\n",
      "  Ref 1:    terdapat wastafel putih di dinding, dengan tanda larangan dan beberapa bahan konstruksi di sekitar. suasana sederhana.\n",
      "Gambar: F044.jpg\n",
      "  Prediksi: sekitarnya dikelilingi pepohonan hijau, suasana di area parkir ini terasa santai dan menenangkan.\n",
      "  Ref 1:    area parkir yang dikelilingi pepohonan, dengan beberapa sepeda motor terparkir dan tanda selamat datang.\n",
      "Gambar: F046.jpg\n",
      "  Prediksi: sebuah jalan setapak berkeramik di depan kiri dan bangunan menjulang di kanan serta pepohonan di depan kiri jalan setapak tersebut\n",
      "  Ref 1:    jalur pejalan kaki dengan pegangan tangan, dikelilingi dinding dan suasana tenang, mengarah ke area terbuka.\n",
      "\n",
      "Total gambar dievaluasi: 301 / 301\n",
      "BLEU-1: 46.32\n",
      "BLEU-2: 28.85\n",
      "BLEU-3: 18.50\n",
      "BLEU-4 (Standar): 11.88\n"
     ]
    }
   ],
   "source": [
    "# GANTI SELURUH CELL 9 (aa57d454) DENGAN KODE INI\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, AutoTokenizer, AutoModelForCausalLM, CLIPVisionModel\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import torch.nn.functional as F\n",
    "import numpy as np \n",
    "\n",
    "# --- 1. KONFIGURASI PATH ---\n",
    "BASE_DIR = \"/home/ubuntu/skripsi/indonesia\"\n",
    "MODEL_PATH = os.path.join(BASE_DIR, \"gemma_adapter_mlp_checkpoint.pt\") \n",
    "TEST_DATA_PATH = os.path.join(BASE_DIR, \"test_captions.csv\")\n",
    "IMAGE_DIR = \"/home/ubuntu/skripsi/indonesia/original\" \n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Menggunakan device: {DEVICE}\")\n",
    "\n",
    "# --- 2. DEFINISI MODEL (HARUS IDENTIK DENGAN TRAINING CELL [7]) ---\n",
    "\n",
    "print(\"Mendefinisikan arsitektur model (sesuai training)...\")\n",
    "\n",
    "# === Adapter yang Dilatih (dari Cell 7) ===\n",
    "class MyAdaptor(nn.Module) :\n",
    "  def __init__(self, vis_token_embedding_size, word_embedding_size) :\n",
    "    super(MyAdaptor, self).__init__()\n",
    "    self.vis_token_embedding_size = vis_token_embedding_size\n",
    "    self.word_embedding_size = word_embedding_size\n",
    "    self.adapter_mlp = nn.Sequential(\n",
    "        nn.Linear(self.vis_token_embedding_size, self.word_embedding_size),\n",
    "        nn.GELU(), # Aktivasi non-linear\n",
    "        nn.Linear(self.word_embedding_size, self.word_embedding_size)\n",
    "    )\n",
    "  def forward(self, img_output) :\n",
    "    img_embed = self.adapter_mlp(img_output)\n",
    "    return img_embed\n",
    "\n",
    "# === Model Utama (dari Cell 7) ===\n",
    "class MyModel(nn.Module) :\n",
    "  def __init__(self) :\n",
    "    super(MyModel, self).__init__()\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        self.model_language = AutoModelForCausalLM.from_pretrained(\n",
    "            \"google/gemma-2-2b-it\", \n",
    "            torch_dtype=torch.bfloat16, \n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "    self.tokenizer_language = AutoTokenizer.from_pretrained(\"google/gemma-2-2b-it\", padding_side= 'right')\n",
    "    self.image_processor = AutoProcessor.from_pretrained(\"openai/clip-vit-base-patch32\").image_processor\n",
    "    self.model_image = CLIPVisionModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(DEVICE)\n",
    "\n",
    "    self.word_embedding_size = self.model_language.config.hidden_size \n",
    "    self.num_vocab = self.model_language.config.vocab_size \n",
    "    self.trigger_str_img = \"<start_image>\"\n",
    "    self.num_vis_token_summary = 50\n",
    "    self.vis_token_embedding_size = self.model_image.config.hidden_size \n",
    "    self.adaptor = MyAdaptor(self.vis_token_embedding_size,self.word_embedding_size )\n",
    "    self.dummy_img_token = (\" \".join([\"the\"]*self.num_vis_token_summary)).strip()\n",
    "\n",
    "  def search_trigger_idx(self, text_token, trigger_str) :\n",
    "    all_token = text_token\n",
    "    all_string_now = \"\"\n",
    "    all_token_now = []\n",
    "    dummy_start_token = None\n",
    "    for token_idx in range(len(all_token)) :\n",
    "      token_now = int(all_token[token_idx].detach().cpu().numpy())\n",
    "      all_token_now.append(token_now)\n",
    "      token_as_string = self.tokenizer_language.batch_decode([all_token_now],skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "      if trigger_str in token_as_string :\n",
    "        dummy_start_token = token_idx + 1\n",
    "        break\n",
    "    return dummy_start_token\n",
    "\n",
    "  def get_image_embed(self, image_input) :\n",
    "    image_input_float = image_input.to(DEVICE, dtype=self.model_image.dtype)\n",
    "    img_output = self.model_image(image_input_float)['last_hidden_state']\n",
    "    img_output_bfloat16 = img_output.to(torch.bfloat16)\n",
    "    img_embed = self.adaptor(img_output_bfloat16)\n",
    "    return img_embed\n",
    "\n",
    "  def split_and_replace(self, now_input_tokens, replacement_embed, start_loc) :\n",
    "    num_token = len(replacement_embed)\n",
    "    start_embed = now_input_tokens[0:start_loc]\n",
    "    end_embed = now_input_tokens[start_loc+num_token:]\n",
    "    replaced_embed = torch.cat((start_embed, replacement_embed.to(now_input_tokens.dtype), end_embed),0)\n",
    "    return replaced_embed\n",
    "\n",
    "  def generate_aswer_image(self, pil_image, max_new_tokens=64):\n",
    "    instruction_now = \"<start_of_turn>user\\n\"\n",
    "    instruction_now += f\"<start_image> {self.dummy_img_token}\\n<end_image>\\n\"\n",
    "    instruction_now += f\"Create a simple description of the image!\\n<end_of_turn>\\n<start_of_turn>model\\n\"\n",
    "    \n",
    "    prompt_tokens = self.tokenizer_language([instruction_now], padding=False, return_tensors=\"pt\")\n",
    "    prompt_tokens = {k: v.to(self.model_language.device) for k, v in prompt_tokens.items()}\n",
    "    \n",
    "    prompt_embeds = self.model_language.model.embed_tokens(prompt_tokens['input_ids'])\n",
    "    \n",
    "    image_input = self.image_processor([pil_image], return_tensors=\"pt\")['pixel_values']\n",
    "    img_embed = self.get_image_embed(image_input) \n",
    "\n",
    "    tokens_text_now = prompt_tokens['input_ids'][0].detach().cpu()\n",
    "    dummy_location = self.search_trigger_idx(tokens_text_now, self.trigger_str_img)\n",
    "\n",
    "    if dummy_location is None:\n",
    "        print(\"WARNING: Tidak bisa menemukan trigger string di prompt.\")\n",
    "        return \"\" \n",
    "\n",
    "    replaced_embeds = self.split_and_replace(prompt_embeds[0], img_embed[0], dummy_location)\n",
    "    replaced_embeds = replaced_embeds.unsqueeze(0) \n",
    "\n",
    "    output_now = self.model_language.generate(\n",
    "        inputs_embeds=replaced_embeds,\n",
    "        attention_mask=prompt_tokens['attention_mask'],\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        num_beams=5,\n",
    "        do_sample=False,\n",
    "        pad_token_id=self.tokenizer_language.eos_token_id\n",
    "    )\n",
    "\n",
    "    # --- INI DIA PERBAIKANNYA ---\n",
    "    # `output_now` HANYA berisi token baru. Kita tidak perlu slicing.\n",
    "    # Kita langsung decode `output_now[0]`\n",
    "    \n",
    "    # BARIS LAMA (SALAH):\n",
    "    # prompt_length = prompt_tokens['input_ids'].shape[1]\n",
    "    # new_tokens = output_now[0, prompt_length:]\n",
    "    # output_string = self.tokenizer_language.decode(new_tokens, ...)\n",
    "    \n",
    "    # BARIS BARU (BENAR):\n",
    "    output_string = self.tokenizer_language.decode(\n",
    "        output_now[0], # Langsung decode output\n",
    "        skip_special_tokens=True, \n",
    "        clean_up_tokenization_spaces=False\n",
    "    )\n",
    "    # -----------------------------\n",
    "    \n",
    "    return output_string.strip()\n",
    "\n",
    "# --- 3. Muat Model dan Checkpoint ---\n",
    "print(\"Inisialisasi model (arsitektur training)...\")\n",
    "model = MyModel()\n",
    "model.adaptor.to(DEVICE, dtype=torch.bfloat16)\n",
    "\n",
    "print(f\"Memuat checkpoint dari: {MODEL_PATH}\")\n",
    "checkpoint = torch.load(MODEL_PATH, map_location=DEVICE)\n",
    "model.adaptor.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "print(f\"Berhasil memuat model dari Step {checkpoint.get('global_step', 'N/A')}\")\n",
    "model.eval() \n",
    "\n",
    "# --- 4. Muat Data Test ---\n",
    "print(f\"Memuat data tes dari: {TEST_DATA_PATH}\")\n",
    "df_test = pd.read_csv(TEST_DATA_PATH, sep='|')\n",
    "\n",
    "print(\"Mengelompokkan referensi caption...\")\n",
    "references_map = {}\n",
    "for img_name, caption in zip(df_test['Image_name'], df_test['caption']): \n",
    "    if img_name not in references_map:\n",
    "        references_map[img_name] = []\n",
    "    references_map[img_name].append(str(caption).lower().split()) \n",
    "    \n",
    "unique_test_images = list(references_map.keys())\n",
    "print(f\"Ditemukan {len(unique_test_images)} gambar unik di test set.\")\n",
    "\n",
    "# --- 5. Jalankan Evaluasi ---\n",
    "print(\"Memulai evaluasi...\")\n",
    "candidates = [] \n",
    "references_list = [] \n",
    "\n",
    "with torch.no_grad():\n",
    "    for img_name in tqdm(unique_test_images, desc=\"Mengevaluasi Model\"):\n",
    "        img_path = os.path.join(IMAGE_DIR, img_name)\n",
    "        \n",
    "        try:\n",
    "            image_raw = Image.open(img_path).convert(\"RGB\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File tidak ditemukan: {img_path}\")\n",
    "            continue\n",
    "            \n",
    "        generated_text = model.generate_aswer_image(\n",
    "            image_raw,\n",
    "            max_new_tokens=64\n",
    "        )\n",
    "        \n",
    "        candidates.append(generated_text.lower().split())\n",
    "        references_list.append(references_map[img_name])\n",
    "\n",
    "# --- 6. Hitung Skor BLEU ---\n",
    "print(\"\\n--- Menghitung BLEU Score ---\")\n",
    "\n",
    "if len(candidates) == 0:\n",
    "    print(\"Evaluasi gagal. Tidak ada kandidat yang dihasilkan.\")\n",
    "else:\n",
    "    print(\"\\n--- Contoh Hasil ---\")\n",
    "    for i in range(min(5, len(candidates))):\n",
    "        print(f\"Gambar: {unique_test_images[i]}\")\n",
    "        print(f\"  Prediksi: {' '.join(candidates[i])}\")\n",
    "        print(f\"  Ref 1:    {' '.join(references_list[i][0])}\")\n",
    "    \n",
    "    bleu1 = corpus_bleu(references_list, candidates, weights=(1.0, 0, 0, 0))\n",
    "    bleu2 = corpus_bleu(references_list, candidates, weights=(0.5, 0.5, 0, 0))\n",
    "    bleu3 = corpus_bleu(references_list, candidates, weights=(0.33, 0.33, 0.33, 0))\n",
    "    bleu4 = corpus_bleu(references_list, candidates, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "    \n",
    "    print(f\"\\nTotal gambar dievaluasi: {len(candidates)} / {len(unique_test_images)}\")\n",
    "    print(f\"BLEU-1: {bleu1*100:.2f}\")\n",
    "    print(f\"BLEU-2: {bleu2*100:.2f}\")\n",
    "    print(f\"BLEU-3: {bleu3*100:.2f}\")\n",
    "    print(f\"BLEU-4 (Standar): {bleu4*100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb72f804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Menggunakan device: cuda\n",
      "Mendefinisikan arsitektur model (Optimized)...\n",
      "Inisialisasi model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "283214ec7cd54e7aa92584f9ee6469a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memuat checkpoint dari: /home/ubuntu/skripsi/indonesia/gemma_adapter_mlp_terbaik.pt\n",
      "Loaded from simple save format\n",
      "Memuat data tes dari: /home/ubuntu/skripsi/indonesia/test_captions.csv\n",
      "Mengelompokkan referensi caption...\n",
      "Ditemukan 301 gambar unik di test set.\n",
      "Memulai evaluasi...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mengevaluasi Model: 100%|██████████| 301/301 [09:44<00:00,  1.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Menghitung BLEU Score ---\n",
      "\n",
      "--- Contoh Hasil ---\n",
      "IMG: F001.jpg\n",
      "  Pred: jalan setapak berbatu di sekitar area parkir yang dikelilingi pepohonan dan tanaman hijau.\n",
      "  Ref:  sepeda motor terparkir rapi di area parkir, sementara dua orang berjalan di latar belakang.\n",
      "--------------------\n",
      "IMG: F008.jpg\n",
      "  Pred: pintu kaca yang terbuka di depan, siap menyambut pengunjung yang akan masuk ke dalam ruangan.\n",
      "  Ref:  naik ke pengalaman baru dengan setiap langkah di tangga ceria ini!\n",
      "--------------------\n",
      "IMG: F031.jpg\n",
      "  Pred: sebuah tempat sampah dengan desain modern dan warna abu-abu, dikelilingi oleh dinding berwarna abu-abu.\n",
      "  Ref:  terdapat wastafel putih di dinding, dengan tanda larangan dan beberapa bahan konstruksi di sekitar. suasana sederhana.\n",
      "--------------------\n",
      "\n",
      "Total gambar dievaluasi: 301\n",
      "BLEU-1: 40.47\n",
      "BLEU-2: 24.73\n",
      "BLEU-3: 15.72\n",
      "BLEU-4: 9.63\n"
     ]
    }
   ],
   "source": [
    "# GANTI SELURUH CELL 9 (EVALUASI BLEU SCORE)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, AutoTokenizer, AutoModelForCausalLM, CLIPVisionModel\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import torch.nn.functional as F\n",
    "import numpy as np \n",
    "\n",
    "# --- 1. KONFIGURASI PATH ---\n",
    "BASE_DIR = \"/home/ubuntu/skripsi/indonesia\"\n",
    "# Pastikan ini mengarah ke file model yang benar (checkpoint atau terbaik)\n",
    "MODEL_PATH = os.path.join(BASE_DIR, \"gemma_adapter_mlp_terbaik.pt\") \n",
    "TEST_DATA_PATH = os.path.join(BASE_DIR, \"test_captions.csv\")\n",
    "IMAGE_DIR = \"/home/ubuntu/skripsi/indonesia/original\" \n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Menggunakan device: {DEVICE}\")\n",
    "\n",
    "# --- 2. DEFINISI MODEL (VERSI OPTIMIZED - SESUAI TRAINING) ---\n",
    "\n",
    "print(\"Mendefinisikan arsitektur model (Optimized)...\")\n",
    "\n",
    "class MyAdaptor(nn.Module) :\n",
    "    def __init__(self, vis_token_embedding_size, word_embedding_size) :\n",
    "        super(MyAdaptor, self).__init__()\n",
    "        self.adapter_mlp = nn.Sequential(\n",
    "            nn.Linear(vis_token_embedding_size, word_embedding_size),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(word_embedding_size, word_embedding_size)\n",
    "        )\n",
    "    def forward(self, img_output) :\n",
    "        return self.adapter_mlp(img_output)\n",
    "\n",
    "class MyModel(nn.Module) :\n",
    "    def __init__(self) :\n",
    "        super(MyModel, self).__init__()\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            self.model_language = AutoModelForCausalLM.from_pretrained(\n",
    "                \"google/gemma-2-2b-it\", \n",
    "                torch_dtype=torch.bfloat16, \n",
    "                device_map=\"auto\"\n",
    "            )\n",
    "        self.tokenizer_language = AutoTokenizer.from_pretrained(\"google/gemma-2-2b-it\", padding_side= 'right')\n",
    "        self.image_processor = AutoProcessor.from_pretrained(\"openai/clip-vit-base-patch32\").image_processor\n",
    "        self.model_image = CLIPVisionModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(DEVICE)\n",
    "\n",
    "        self.word_embedding_size = self.model_language.config.hidden_size \n",
    "        self.num_vocab = self.model_language.config.vocab_size \n",
    "        \n",
    "        # --- OPTIMASI TOKEN TRIGGER (Sama seperti Training) ---\n",
    "        self.trigger_str_img = \"<start_image>\"\n",
    "        self.trigger_token_id = self.tokenizer_language.encode(self.trigger_str_img, add_special_tokens=False)[-1]\n",
    "        \n",
    "        self.num_vis_token_summary = 50\n",
    "        self.vis_token_embedding_size = self.model_image.config.hidden_size \n",
    "        self.adaptor = MyAdaptor(self.vis_token_embedding_size,self.word_embedding_size )\n",
    "        self.dummy_img_token = (\" \".join([\"the\"]*self.num_vis_token_summary)).strip()\n",
    "\n",
    "    # --- PENCARIAN CEPAT (GPU) ---\n",
    "    def search_trigger_idx(self, input_ids_tensor) :\n",
    "        matches = (input_ids_tensor == self.trigger_token_id).nonzero(as_tuple=True)[0]\n",
    "        if len(matches) > 0:\n",
    "            return matches[0].item() + 1\n",
    "        return None\n",
    "\n",
    "    def get_image_embed(self, image_input) :\n",
    "        image_input_float = image_input.to(DEVICE, dtype=self.model_image.dtype)\n",
    "        img_output = self.model_image(image_input_float)['last_hidden_state']\n",
    "        img_output_bfloat16 = img_output.to(torch.bfloat16)\n",
    "        img_embed = self.adaptor(img_output_bfloat16)\n",
    "        return img_embed\n",
    "\n",
    "    def split_and_replace(self, now_input_tokens, replacement_embed, start_loc) :\n",
    "        num_token = len(replacement_embed)\n",
    "        start_embed = now_input_tokens[0:start_loc]\n",
    "        end_embed = now_input_tokens[start_loc+num_token:]\n",
    "        replaced_embed = torch.cat((start_embed, replacement_embed.to(now_input_tokens.dtype), end_embed),0)\n",
    "        return replaced_embed\n",
    "\n",
    "    def generate_aswer_image(self, pil_image, max_new_tokens=64):\n",
    "        # 1. Prepare Prompt\n",
    "        instruction_now = \"<start_of_turn>user\\n\"\n",
    "        instruction_now += f\"<start_image> {self.dummy_img_token}\\n<end_image>\\n\"\n",
    "        instruction_now += f\"Create a simple description of the image!\\n<end_of_turn>\\n<start_of_turn>model\\n\"\n",
    "        \n",
    "        prompt_tokens = self.tokenizer_language([instruction_now], padding=False, return_tensors=\"pt\")\n",
    "        input_ids = prompt_tokens['input_ids'].to(DEVICE)\n",
    "        attention_mask = prompt_tokens['attention_mask'].to(DEVICE)\n",
    "        \n",
    "        # 2. Embed Prompt (Text)\n",
    "        prompt_embeds = self.model_language.model.embed_tokens(input_ids)\n",
    "        \n",
    "        # 3. Embed Image (Vision)\n",
    "        image_input = self.image_processor([pil_image], return_tensors=\"pt\")['pixel_values']\n",
    "        img_embed = self.get_image_embed(image_input) \n",
    "\n",
    "        # 4. Find Trigger & Replace (Optimized)\n",
    "        dummy_location = self.search_trigger_idx(input_ids[0]) # Pass tensor directly\n",
    "\n",
    "        if dummy_location is None:\n",
    "            return \"Error: Trigger token not found\"\n",
    "\n",
    "        replaced_embeds = self.split_and_replace(prompt_embeds[0], img_embed[0], dummy_location)\n",
    "        replaced_embeds = replaced_embeds.unsqueeze(0) \n",
    "\n",
    "        # 5. Generate\n",
    "        output_ids = self.model_language.generate(\n",
    "            inputs_embeds=replaced_embeds,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            num_beams=3, # Beam search kecil untuk kualitas\n",
    "            do_sample=False,\n",
    "            pad_token_id=self.tokenizer_language.eos_token_id\n",
    "        )\n",
    "\n",
    "        # 6. Decode Output\n",
    "        # output_ids[0] berisi token hasil generate saja (karena inputs_embeds dipakai)\n",
    "        output_string = self.tokenizer_language.decode(\n",
    "            output_ids[0], \n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "        \n",
    "        return output_string.strip()\n",
    "\n",
    "# --- 3. Muat Model dan Checkpoint ---\n",
    "print(\"Inisialisasi model...\")\n",
    "model = MyModel()\n",
    "model.adaptor.to(DEVICE, dtype=torch.bfloat16)\n",
    "\n",
    "print(f\"Memuat checkpoint dari: {MODEL_PATH}\")\n",
    "# Load checkpoint: Handle jika yang disimpan hanya state_dict adapter atau full checkpoint\n",
    "if os.path.exists(MODEL_PATH):\n",
    "    checkpoint = torch.load(MODEL_PATH, map_location=DEVICE)\n",
    "    \n",
    "    # Cek struktur checkpoint\n",
    "    if 'model_state_dict' in checkpoint:\n",
    "        model.adaptor.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(f\"Loaded from checkpoint (Epoch {checkpoint.get('epoch', '?')})\")\n",
    "    elif 'model' in checkpoint:\n",
    "        model.adaptor.load_state_dict(checkpoint['model']) # Handle format lama\n",
    "        print(\"Loaded from simple save format\")\n",
    "    else:\n",
    "        # Asumsi file langsung berisi state_dict\n",
    "        model.adaptor.load_state_dict(checkpoint)\n",
    "        print(\"Loaded state_dict directly\")\n",
    "else:\n",
    "    print(f\"CRITICAL: File model tidak ditemukan di {MODEL_PATH}\")\n",
    "\n",
    "model.eval() \n",
    "\n",
    "# --- 4. Muat Data Test ---\n",
    "print(f\"Memuat data tes dari: {TEST_DATA_PATH}\")\n",
    "df_test = pd.read_csv(TEST_DATA_PATH, sep='|')\n",
    "\n",
    "print(\"Mengelompokkan referensi caption...\")\n",
    "references_map = {}\n",
    "for img_name, caption in zip(df_test['Image_name'], df_test['caption']): \n",
    "    if img_name not in references_map:\n",
    "        references_map[img_name] = []\n",
    "    references_map[img_name].append(str(caption).lower().split()) \n",
    "    \n",
    "unique_test_images = list(references_map.keys())\n",
    "print(f\"Ditemukan {len(unique_test_images)} gambar unik di test set.\")\n",
    "\n",
    "# --- 5. Jalankan Evaluasi ---\n",
    "print(\"Memulai evaluasi...\")\n",
    "candidates = [] \n",
    "references_list = [] \n",
    "\n",
    "# Kita batasi sample untuk debugging cepat (opsional), hapus slice [:10] untuk full test\n",
    "test_images_to_run = unique_test_images \n",
    "\n",
    "with torch.no_grad():\n",
    "    for img_name in tqdm(test_images_to_run, desc=\"Mengevaluasi Model\"):\n",
    "        img_path = os.path.join(IMAGE_DIR, img_name)\n",
    "        \n",
    "        try:\n",
    "            image_raw = Image.open(img_path).convert(\"RGB\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Skip: File tidak ditemukan {img_path}\")\n",
    "            continue\n",
    "            \n",
    "        generated_text = model.generate_aswer_image(\n",
    "            image_raw,\n",
    "            max_new_tokens=64\n",
    "        )\n",
    "        \n",
    "        candidates.append(generated_text.lower().split())\n",
    "        references_list.append(references_map[img_name])\n",
    "\n",
    "# --- 6. Hitung Skor BLEU ---\n",
    "print(\"\\n--- Menghitung BLEU Score ---\")\n",
    "\n",
    "if len(candidates) == 0:\n",
    "    print(\"Evaluasi gagal. Tidak ada kandidat yang dihasilkan.\")\n",
    "else:\n",
    "    print(\"\\n--- Contoh Hasil ---\")\n",
    "    for i in range(min(3, len(candidates))):\n",
    "        print(f\"IMG: {test_images_to_run[i]}\")\n",
    "        print(f\"  Pred: {' '.join(candidates[i])}\")\n",
    "        print(f\"  Ref:  {' '.join(references_list[i][0])}\")\n",
    "        print(\"-\" * 20)\n",
    "    \n",
    "    # Smoothing function untuk menghindari skor 0 jika kalimat pendek\n",
    "    from nltk.translate.bleu_score import SmoothingFunction\n",
    "    cc = SmoothingFunction()\n",
    "\n",
    "    bleu1 = corpus_bleu(references_list, candidates, weights=(1.0, 0, 0, 0), smoothing_function=cc.method1)\n",
    "    bleu2 = corpus_bleu(references_list, candidates, weights=(0.5, 0.5, 0, 0), smoothing_function=cc.method1)\n",
    "    bleu3 = corpus_bleu(references_list, candidates, weights=(0.33, 0.33, 0.33, 0), smoothing_function=cc.method1)\n",
    "    bleu4 = corpus_bleu(references_list, candidates, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=cc.method1)\n",
    "    \n",
    "    print(f\"\\nTotal gambar dievaluasi: {len(candidates)}\")\n",
    "    print(f\"BLEU-1: {bleu1*100:.2f}\")\n",
    "    print(f\"BLEU-2: {bleu2*100:.2f}\")\n",
    "    print(f\"BLEU-3: {bleu3*100:.2f}\")\n",
    "    print(f\"BLEU-4: {bleu4*100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4197fd79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a4f2cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skripsi_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
